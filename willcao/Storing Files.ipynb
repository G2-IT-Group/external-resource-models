{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "### purpose is to save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-46bc4af01941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfernet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFernet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "# the purpose of this program is to parse through all the \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import re \n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sn\n",
    "from cryptography.fernet import Fernet\n",
    "import pyarrow as pa\n",
    "\n",
    "import parquet\n",
    "#from fastparquet import write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "initial_path = \"/data/data\"\n",
    "root_dir = os.listdir(initial_path)\n",
    "list_of_json = []\n",
    "counter_total = 0;\n",
    "for dir in root_dir:\n",
    "    curr_path = initial_path + \"/\" + dir + \"/enrichment/dns\"\n",
    "    #print(curr_dir)\n",
    "    if os.path.exists(curr_path):\n",
    "        curr_dir = os.listdir(curr_path)\n",
    "        for sub_json in curr_dir:\n",
    "        #do i need the last /\n",
    "            each_json = curr_path + \"/\" + sub_json\n",
    "            print(each_json)\n",
    "            #ran =random.random()\n",
    "            #if(ran<.1.01):\n",
    "            counter_total = counter_total+1\n",
    "                #print('counter_total'+ str(counter_total))\n",
    "            list_of_json.append(each_json)\n",
    "    else:\n",
    "        pass    \n",
    "print('counter_total'+ str(counter_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = []\n",
    "a_df = pd.DataFrame()\n",
    "target_function_list_origional_copy = []\n",
    "list_of_bad_json = []\n",
    "for a_json in list_of_json:\n",
    "    print('Test')\n",
    "    with open(''+a_json) as data_file:\n",
    "        try:             \n",
    "            data = json.load(data_file)\n",
    "            data_frame = json_normalize(data['data'])\n",
    "            print ((len(a_json)))\n",
    "           # print ((len(data_frame.index)))\n",
    "            if((len(a_json)<60)==True):\n",
    "                print('goooooooddddd')\n",
    "                for x in range(0,len(data_frame.index),1):\n",
    "                    target_function_list_origional_copy.append(1)\n",
    "                a_df = a_df.append(data_frame)\n",
    "            if((len(a_json)>=60)==True):\n",
    "                ran = random.random()\n",
    "                if( ran < .1.01):\n",
    "                    print('badd')\n",
    "                    for x in range(0,len(data_frame.index),1):\n",
    "                        target_function_list_origional_copy.append(0)             \n",
    "                    a_df = a_df.append(data_frame)\n",
    "            #print(\"target_function_list length\" + len(target_function_list))\n",
    "\n",
    "        except:\n",
    "           \n",
    "            list_of_bad_json.append(a_json)\n",
    "            continue\n",
    "print(\"done\" )        \n",
    "print(len(a_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df['target']=target_function_list_origional_copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
