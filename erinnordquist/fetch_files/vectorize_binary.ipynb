{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the files that were not used to create the word list in the find_words notebook and vectorizes them so they can be fed through models. Still working on a way to effectively safe this numerical data, but I may only be able to do that with binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pandas.io.json import json_normalize\n",
    "import cryptography\n",
    "import re\n",
    "from cryptography.fernet import Fernet\n",
    "import bs4\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def tokenize(text):\n",
    "    text = re.sub('&quot;','\\'',text)\n",
    "    text = re.sub('&amp;','&',text)\n",
    "    text = re.sub('\"', '', text)\n",
    "    text = re.sub(r'\\'(?!\\')(.*)\\'',r'\\0', text)\n",
    "    \n",
    "    toks = re.split(r'[\\s,:;{}\\(\\)<>\\/ï¿½]+',text.lower())\n",
    "    return sorted(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [a for a,b in load_obj('sorted_kvpairs')]\n",
    "vocab = sorted(vocab[:10000])#takes the top 10000 by freq and puts them in alphabetical/ascii order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------newNONTORbenigndata---------------\n",
      "------------2019-07-01---------------\n",
      "------------2019-07-02---------------\n",
      "------------2019-07-03---------------\n",
      "------------2019-07-07---------------\n",
      "------------2019-06-22---------------\n",
      "------------2019-06-23---------------\n",
      "------------2019-06-25---------------\n",
      "------------2019-06-26---------------\n",
      "------------2019-06-27---------------\n",
      "------------2019-06-28---------------\n",
      "------------2019-06-29---------------\n",
      "------------2019-06-30---------------\n",
      "------------2019-06-14---------------\n",
      "------------2019-06-15---------------\n",
      "------------2019-06-16---------------\n",
      "------------2019-06-17---------------\n",
      "------------2019-06-18---------------\n",
      "------------2019-06-19---------------\n",
      "------------2019-06-20---------------\n",
      "------------2019-06-21---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create corpus\n",
    "data = []\n",
    "labels = []\n",
    "indicators=[]\n",
    "# Not included as all their files seem to be empty or they have no enrichment: \n",
    "    #4/5, 4/6, 4/13, 4/14, 4/15, 4/31, 5/13, 5/14, 5/15, 5/16, 5/17, 5/18, 7/4, 7/5, 7/6\n",
    "# Empty List serves as a place holder between the data used in find_words and the test data I'm using here\n",
    "date_batches = [['newbenigndata'],['2019-04-22','2019-04-23','2019-04-24','2019-04-25','2019-04-26','2019-04-27','2019-04-28','2019-04-29'],\n",
    "                ['2019-04-11','2019-04-12','2019-04-16','2019-04-17','2019-04-18','2019-04-19','2019-04-20','2019-04-21'],\n",
    "                ['2019-04-30','2019-05-01','2019-05-02','2019-05-03','2019-05-04'],\n",
    "                ['2019-05-05','2019-05-06','2019-05-07','2019-05-08','2019-05-09','2019-05-10','2019-05-11','2019-05-12'],\n",
    "                ['2019-05-19','2019-05-20'],\n",
    "                ['2019-05-21','2019-05-22','2019-05-23','2019-05-24','2019-05-25','2019-05-26','2019-05-27','2019-05-28'],\n",
    "                ['2019-05-29','2019-05-30','2019-05-31','2019-06-01','2019-06-02','2019-06-03','2019-06-04','2019-06-05'],\n",
    "                ['2019-06-06','2019-06-07','2019-06-08','2019-06-09','2019-06-10','2019-06-11','2019-06-12','2019-06-13'],[],\n",
    "                ['2019-06-14', '2019-06-15','2019-06-16', '2019-06-17','2019-06-18','2019-06-19', '2019-06-20', '2019-06-21'],\n",
    "                ['2019-06-22', '2019-06-23', '2019-06-25', '2019-06-26', '2019-06-27', '2019-06-28','2019-06-29', '2019-06-30'],\n",
    "                ['2019-07-01','2019-07-02', '2019-07-03', '2019-07-07'],\n",
    "                ['newNONTORbenigndata']]\n",
    "dates = date_batches[-1]+date_batches[-2]+date_batches[-3]+date_batches[-4]#+date_batches[3]+date_batches[4]+date_batches[5]+date_batches[6]+date_batches[7]+date_batches[8] \n",
    "#'2019-04-12','2019-04-16','2019-04-17','2019-04-18','2019-04-19','2019-04-20','2019-04-21]\n",
    "for date in dates:\n",
    "    files = os.listdir('/data/data/'+date+'/enrichment/fetch_page')\n",
    "    top = len(files)\n",
    "    if top > 70:\n",
    "        top = 70\n",
    "    for j in range(top):\n",
    "        with open('/data/data/'+date+'/enrichment/fetch_page/'+files[j]) as d:\n",
    "            r = json.load(d)\n",
    "            try:\n",
    "                df = json_normalize(r['data'])\n",
    "            except:\n",
    "                break\n",
    "        try:\n",
    "            df = df[df['status_code'] == 200].reset_index(drop=True)\n",
    "        except:\n",
    "            df = df[df['success']]\n",
    "        #Need to determine label\n",
    "        html_str = df['page_content'].copy()\n",
    "        \n",
    "        for i in range(len(html_str)):\n",
    "            if df.loc[i,'indicator'] in indicators:\n",
    "                continue\n",
    "            else:\n",
    "                indicators.append(df.loc[i, 'indicator'])\n",
    "            if len(files[j]) > 10:\n",
    "                labels += [1]\n",
    "            else:\n",
    "                labels += [0]\n",
    "            key = bytes( df.loc[i,'encryption_key'],encoding = 'UTF-8')\n",
    "            f = Fernet(key) \n",
    "            #decrypt string\n",
    "            text = f.decrypt(bytes(html_str[i],encoding='UTF-8'))\n",
    "            #clean out html\n",
    "            text = text.decode('utf8')\n",
    "            toks = tokenize(text)\n",
    "            #need to create vector \n",
    "            count = Counter()\n",
    "            toks = set(toks)\n",
    "            for tok in toks:\n",
    "                count[tok] += 1\n",
    "            vec = []\n",
    "            for v in vocab:\n",
    "                vec = np.append(vec, [count[v]])\n",
    "            data.append(vec)\n",
    "                \n",
    "            \n",
    "            \n",
    "         #end of for\n",
    "    #end of for\n",
    "    print('------------'+date+'---------------')\n",
    "#end of for\n",
    "data = np.row_stack(data)\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4440, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4440, 10001)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "np.column_stack([data,labels]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2416, 10001)\n",
      "(2024, 10001)\n",
      "(2025, 10001)\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame( data,columns=vocab)#np.column_stack([data,labels])\n",
    "X['unsafe'] = labels\n",
    "X1 = X[X['unsafe']==1].sample(frac=1)\n",
    "print(X1.shape)\n",
    "X0 = X[X['unsafe']==0]\n",
    "print(X0.shape)\n",
    "X1 = X1.reset_index(drop=True).loc[:(X0.shape[0]),:]\n",
    "print(X1.shape)\n",
    "temp_df = pd.concat([X0,X1]).sample(frac=1)\n",
    "X = temp_df.drop(columns=['unsafe'])\n",
    "Y = temp_df['unsafe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\u0000</th>\n",
       "      <th>\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</th>\n",
       "      <th>...</th>\n",
       "      <th>ï½</th>\n",
       "      <th>ï</th>\n",
       "      <th>ï¢</th>\n",
       "      <th>ï¢¥</th>\n",
       "      <th>ï¤</th>\n",
       "      <th>ï»¿</th>\n",
       "      <th>ð±¤¹Ç°\u0001</th>\n",
       "      <th>ñ°</th>\n",
       "      <th>ó¿¬</th>\n",
       "      <th>ô´[`kx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       \u0000   \u0000\u0000  \u0000\u0000\u0000  \u0000\u0000\u0000\u0000  \u0000\u0000\u0000\u0000\u0000  \u0000\u0000\u0000\u0000\u0000\u0000  \u0000\u0000\u0000\u0000\u0000\u0000\u0000  \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000  \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000  \\\n",
       "533  0.0  0.0  0.0   0.0    0.0     0.0      0.0       0.0           0.0   \n",
       "567  1.0  0.0  0.0   0.0    0.0     0.0      0.0       0.0           0.0   \n",
       "590  0.0  0.0  0.0   0.0    0.0     0.0      0.0       0.0           0.0   \n",
       "251  0.0  0.0  0.0   0.0    0.0     0.0      0.0       0.0           0.0   \n",
       "695  0.0  0.0  0.0   0.0    0.0     0.0      0.0       0.0           0.0   \n",
       "\n",
       "     \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000  ...      ï½    ï    ï¢    ï¢¥    ï¤    ï»¿  ð±¤¹Ç°\u0001    ñ°    ó¿¬  ô´[`kx  \n",
       "533             0.0  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "567             0.0  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "590             0.0  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "251             0.0  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "695             0.0  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8720987654320987\n",
      "('.gettime', 0.007211518004171516)\n",
      "('name=description', 0.006964242608226962)\n",
      "('href=https', 0.006883216882838714)\n",
      "('script', 0.005643908890702067)\n",
      "('new', 0.005445354979575827)\n",
      "('s', 0.005336315271883909)\n",
      "('||', 0.005226819611805348)\n",
      "('hidden', 0.005191186795500147)\n",
      "('w', 0.004951281648809325)\n",
      "('\\x00', 0.004854739156144193)\n",
      "('date', 0.004749199584284138)\n",
      "('style=display', 0.0046991893459875245)\n",
      "('13', 0.004670589557140938)\n",
      "('[]', 0.004586772419947345)\n",
      "('10', 0.004415773903514908)\n",
      "('span', 0.004214632501993295)\n",
      "('[0]', 0.004162046450526997)\n",
      "('property=fb', 0.003974566316996299)\n",
      "('target=_blank', 0.003972647003254523)\n",
      "('2019', 0.0037979135797544597)\n",
      "('!doctype', 0.003738887617814578)\n",
      "('|', 0.0035601806450565677)\n",
      "('url', 0.0035507324140240672)\n",
      "('complete===b.readystate&&c.readycallback', 0.0033807259121708434)\n",
      "('image', 0.003337402382259342)\n",
      "('function', 0.0031661923576471315)\n",
      "('false', 0.0030964688346318686)\n",
      "('s.w.org\\\\', 0.0029543226588493493)\n",
      "('1', 0.0028983871492436527)\n",
      "('name=keywords', 0.0028847401230173023)\n",
      "('document.createelement', 0.002850668625393303)\n",
      "(']', 0.0028304464497694846)\n",
      "('null', 0.002816828399607416)\n",
      "('world', 0.002755141228972805)\n",
      "('html', 0.002659484046549651)\n",
      "('news', 0.0026455768870924288)\n",
      "('twitter', 0.0026387347968264744)\n",
      "('true', 0.002602405921936112)\n",
      "('from', 0.0025102109060212018)\n",
      "('iframe', 0.002457506256216737)\n",
      "('content=https', 0.0024362374885172197)\n",
      "('border-spacing', 0.0024338119369611355)\n",
      "('videos', 0.0023453178598841578)\n",
      "('noscript', 0.002330383964130214)\n",
      "('site', 0.0023061994527450924)\n",
      "('apps', 0.002217199692319886)\n",
      "('in', 0.0022171760633018854)\n",
      "('search', 0.0021994523754568867)\n",
      "('www.youtube.com', 0.002197209433960738)\n",
      "('21', 0.0021915322414285477)\n",
      "('favicon.ico', 0.0021555576393690787)\n",
      "('ld+json', 0.0021146645043535937)\n",
      "('&gt', 0.002092634328819653)\n",
      "('a', 0.002083619422734486)\n",
      "('charset=utf-8', 0.0020709722618931334)\n",
      "('terms', 0.002068352452308222)\n",
      "('rel=canonical', 0.0020505142848473617)\n",
      "('56128', 0.001992524885185081)\n",
      "('arguments', 0.001986291609959231)\n",
      "('meta', 0.001964673353154758)\n",
      "('us', 0.0019518477488722201)\n",
      "('56826', 0.001947236794769091)\n",
      "('privacy', 0.0019427787985629985)\n",
      "('portal', 0.0019329360790932225)\n",
      "('=', 0.001927115048199836)\n",
      "('7', 0.0019214575146629792)\n",
      "('async', 0.0019123577064284966)\n",
      "('https', 0.0018889402549056799)\n",
      "('wlwmanifest.xml', 0.0018878056070687117)\n",
      "('var', 0.001870547318609991)\n",
      "('ul', 0.001866075733339426)\n",
      "('js', 0.0018597280699304119)\n",
      "('map', 0.0018494830281874115)\n",
      "('56423', 0.0018492414811900064)\n",
      "('www.facebook.com', 0.0018427347315672603)\n",
      "('td', 0.0018196984719267228)\n",
      "('l', 0.0018149248766994495)\n",
      "('menu-item-type-post_type', 0.0018029116430985357)\n",
      "('join', 0.0017909881816974382)\n",
      "('else', 0.0017905105749573317)\n",
      "('16', 0.0017897333633485348)\n",
      "('head', 0.0017854110229929981)\n",
      "('oembed', 0.0017459855944840738)\n",
      "('rel=stylesheet', 0.0017438851596222357)\n",
      "('property=og', 0.0017413935409839835)\n",
      "('17', 0.001737567443935012)\n",
      "('href=http', 0.0017031755468967125)\n",
      "('event', 0.0016775803561492222)\n",
      "('19', 0.0016686575654916093)\n",
      "('cursor', 0.0016578303115553736)\n",
      "('width=1', 0.0016516666113003595)\n",
      "('app', 0.0016387015727563122)\n",
      "('tahoma', 0.0016090156906678446)\n",
      "('use', 0.0015955913536518731)\n",
      "('document', 0.0015913810482061385)\n",
      "('2', 0.0015894253520869542)\n",
      "('window._wpemojisettings', 0.00158568463867204)\n",
      "('youtube', 0.0015801278147841665)\n",
      "('12', 0.0015719748032269575)\n",
      "('k=b.createelement', 0.0015639479746006764)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.88      1038\n",
      "          1       0.87      0.87      0.87       987\n",
      "\n",
      "avg / total       0.87      0.87      0.87      2025\n",
      "\n",
      "0.6252517962976133\n",
      "('[0]', 0.1133533281846654)\n",
      "('name=description', 0.06909952981589494)\n",
      "('border-spacing', 0.03871275936029375)\n",
      "('date', 0.03140565369946959)\n",
      "('href=https', 0.031354530414216576)\n",
      "('s', 0.011723798659561972)\n",
      "('news', 0.009351462296716342)\n",
      "('name=keywords', 0.00795866699246368)\n",
      "('type=image', 0.00790155986084631)\n",
      "('domains', 0.007549978468090667)\n",
      "('menu-item-object-page', 0.006146320567933012)\n",
      "('2d', 0.005898768994856945)\n",
      "('lang=ru', 0.005599580655633836)\n",
      "('tahoma', 0.005443302554636033)\n",
      "('current_page_item', 0.005327825859765106)\n",
      "('menu-item-type-post_type', 0.005275189849710487)\n",
      "('&raquo', 0.005183676548998837)\n",
      "('svg\\\\', 0.005070416069734276)\n",
      "('rel=canonical', 0.005020569794346714)\n",
      "('style.css', 0.004536393769222996)\n",
      "('twitter.com', 0.004286823852322394)\n",
      "('script', 0.004014112867286848)\n",
      "('rel=\\x00', 0.0038230599209676955)\n",
      "('input', 0.003663452265496526)\n",
      "('privacy', 0.0036545492007793077)\n",
      "('!doctype', 0.003650740819152891)\n",
      "('ÑÐ¾ÑÑÐ¸Ð½Ð³', 0.003497329234452)\n",
      "('hidden', 0.003312019438946211)\n",
      "('head', 0.0031230329136122657)\n",
      "('title=rsd', 0.0030212633227426266)\n",
      "('portal', 0.002994353033368411)\n",
      "('wp-includes\\\\', 0.00277336850992052)\n",
      "('1', 0.0026259760178780974)\n",
      "('class=fa', 0.0026040956294374872)\n",
      "('page_item', 0.002475901277662438)\n",
      "('json+oembed', 0.0024186111864312417)\n",
      "('xmlrpc.php?rsd', 0.002404185334309744)\n",
      "('[]', 0.002365207695647366)\n",
      "('s.w.org\\\\', 0.002336756590969822)\n",
      "('search', 0.0022840723414410723)\n",
      "('sex', 0.0021272011359407756)\n",
      "('border=0', 0.0021069797068124785)\n",
      "('colors', 0.0020655027227456575)\n",
      "('|', 0.0020541151096863065)\n",
      "('http-equiv=refresh', 0.0020529568186211334)\n",
      "('world.', 0.002034860277007961)\n",
      "('ld+json', 0.0020277536348454195)\n",
      "('special', 0.0019915741362657633)\n",
      "('app', 0.0019512675483806898)\n",
      "('height=1', 0.0019392403963312335)\n",
      "('online', 0.001908883180424421)\n",
      "('game', 0.0019005477037459066)\n",
      "('hs', 0.001898561695279034)\n",
      "('sizes=180x180', 0.0018727351849086387)\n",
      "('youtube', 0.001855689367984335)\n",
      "('cursor', 0.0018331741745994383)\n",
      "('link', 0.0018314573592031719)\n",
      "('android', 0.0018248987626249026)\n",
      "('ÑÐ¾ÑÑÐ¸Ð¸', 0.0017924161934028374)\n",
      "('pointer', 0.0017740999779672646)\n",
      "('black', 0.0017388546295043362)\n",
      "('Ð¿Ð¾Ð´', 0.0017246621826284485)\n",
      "('async=async', 0.0016577937245590608)\n",
      "('builder', 0.0016488289063910134)\n",
      "('href=css', 0.0016443732741846125)\n",
      "('style=display', 0.0016379027892988735)\n",
      "('2019', 0.0016365912210771273)\n",
      "('wordpress', 0.001634678857874042)\n",
      "('property=fb', 0.0016343677083496)\n",
      "('longer', 0.001629150728672122)\n",
      "('format=xml', 0.0016220803025868743)\n",
      "('wp-content', 0.0015713629073084417)\n",
      "('bootstrap', 0.0015188978267752313)\n",
      "('56423', 0.0015049181458147898)\n",
      "('11px', 0.0014928853551357505)\n",
      "('emoji\\\\', 0.0014850689750231928)\n",
      "('middle', 0.001468565148532779)\n",
      "('careers', 0.001452164796846379)\n",
      "('user', 0.0014301431912513887)\n",
      "('page-template-default', 0.0014126200910659047)\n",
      "('class=site-title', 0.0014047640704571971)\n",
      "('wp-json', 0.0013968221536649352)\n",
      "('domcontentloaded', 0.0013900331978402192)\n",
      "('continue', 0.0013844567483462628)\n",
      "('.gettime', 0.0013729383947905685)\n",
      "('debug', 0.0013599347878781812)\n",
      "('www.youtube.com', 0.0013576759759955685)\n",
      "('voor', 0.0013436980462963142)\n",
      "('width=88', 0.0013330108266795212)\n",
      "('http-equiv=x-ua-compatible', 0.0013279619388141327)\n",
      "('autocomplete=off', 0.0013149849234193101)\n",
      "('france', 0.001314946321255828)\n",
      "('2', 0.0013117778540446376)\n",
      "(']', 0.0012785457821519626)\n",
      "('here', 0.001269485175194264)\n",
      "('-10px', 0.0012522355685769724)\n",
      "('null', 0.0012470330867187303)\n",
      "('escape', 0.00124606362602875)\n",
      "('div', 0.0012441154257908124)\n",
      "('rel=apple-touch-icon', 0.001222652220208764)\n"
     ]
    }
   ],
   "source": [
    "#X = data\n",
    "#Y = labels\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size= 0.5, test_size=0.5, random_state=13)\n",
    "forest = RandomForestClassifier(n_estimators=80,random_state=13)\n",
    "forest.fit(X_train,Y_train)\n",
    "print()\n",
    "print(forest.score(X_test,Y_test))\n",
    "print(*(sorted(list(zip(vocab,forest.feature_importances_)), key=lambda x: x[1],reverse=True))[:100], sep = '\\n')\n",
    "print(classification_report(Y_test,forest.predict(X_test)))\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=80,random_state=13)\n",
    "forest.fit(X_train,Y_train)\n",
    "print(forest.score(X_test,Y_test))\n",
    "print(*(sorted(list(zip(vocab,forest.feature_importances_)), key=lambda x: x[1],reverse=True))[:100], sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8864197530864197\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.89      1038\n",
      "          1       0.86      0.92      0.89       987\n",
      "\n",
      "avg / total       0.89      0.89      0.89      2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=13)\n",
    "clf.fit(X_train,Y_train)\n",
    "print(clf.score(X_test,Y_test))\n",
    "print(classification_report(Y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40684114\n",
      "Iteration 2, loss = 0.16695831\n",
      "Iteration 3, loss = 0.09421176\n",
      "Iteration 4, loss = 0.06579691\n",
      "Iteration 5, loss = 0.05165654\n",
      "Iteration 6, loss = 0.05030142\n",
      "Iteration 7, loss = 0.04644707\n",
      "Iteration 8, loss = 0.05243231\n",
      "Iteration 9, loss = 0.06037857\n",
      "Iteration 10, loss = 0.06852215\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "0.8725925925925926\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.80      0.87      1038\n",
      "          1       0.82      0.95      0.88       987\n",
      "\n",
      "avg / total       0.88      0.87      0.87      2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(verbose=True, batch_size= 11, random_state = 13)\n",
    "clf.fit(X_train,Y_train)\n",
    "print(clf.score(X_test,Y_test))\n",
    "print(classification_report(Y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42246430\n",
      "Iteration 2, loss = 0.19735366\n",
      "Iteration 3, loss = 0.12187240\n",
      "Iteration 4, loss = 0.07751181\n",
      "Iteration 5, loss = 0.05450115\n",
      "Iteration 6, loss = 0.04955718\n",
      "Iteration 7, loss = 0.04400787\n",
      "Iteration 8, loss = 0.04256621\n",
      "Iteration 9, loss = 0.03743643\n",
      "Iteration 10, loss = 0.04482492\n",
      "Iteration 11, loss = 0.04163082\n",
      "Iteration 12, loss = 0.03904210\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "0.8784909358157765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.81      0.87      1023\n",
      "          1       0.83      0.94      0.89      1018\n",
      "\n",
      "avg / total       0.89      0.88      0.88      2041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(verbose=True, batch_size= 12, random_state = 13)\n",
    "clf.fit(X_train,Y_train)\n",
    "print(clf.score(X_test,Y_test))\n",
    "print(classification_report(Y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
