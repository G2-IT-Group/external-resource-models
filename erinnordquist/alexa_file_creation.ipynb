{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to sample part of the data set, change dates to a list of dates or use the commented code to take a mostly-random sample \n",
    "#running this block with no changes will load the entire dataset\n",
    "\n",
    "all_dates =  ['newNONTORbenigndata'] + os.listdir('/data/data')[1:-3]\n",
    "dates = all_dates\n",
    "'''\n",
    "dates = []\n",
    "sample_size = 250\n",
    "for i in range(sample_size):\n",
    "    randv = np.random.rand(0,len(all_dates))\n",
    "    try:\n",
    "        dates.append(all_dates.pop(randv))\n",
    "    except: \n",
    "        dates.append(all_dates.pop())\n",
    "'''\n",
    "df_lst = []\n",
    "unsafe_lst = []\n",
    "unsafe_req_ids=[]\n",
    "safe_lst = []\n",
    "safe_req_ids=[]\n",
    "for d in dates:\n",
    "    try:\n",
    "        date = d\n",
    "        enrichment_path = '/data/data/'+date+'/enrichment'\n",
    "        alexa_path = enrichment_path+'/alexa'\n",
    "        files = os.listdir(alexa_path)\n",
    "        print(date)\n",
    "        for i in range(len(files)):\n",
    "            with open(alexa_path+'/'+files[i]) as d:\n",
    "                try:\n",
    "                    r=json.load(d)\n",
    "\n",
    "                    try:\n",
    "                        temp_df=json_normalize(r['data'])\n",
    "                        labels = []\n",
    "                        if len(files[i]) <= 10 :\n",
    "                            labels += [0]*len(temp_df)\n",
    "                        else :\n",
    "                            labels += [1]*len(temp_df)\n",
    "                        temp_df['unsafe'] = labels\n",
    "                        df_lst.append(temp_df)\n",
    "                       \n",
    "                    except:\n",
    "                        pass\n",
    "                except:\n",
    "                    print(date)\n",
    "    except:\n",
    "        print('something broke')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring all frames together and drop an empty column\n",
    "try:\n",
    "    df = pd.concat(df_lst, axis = 0, join = 'outer', ignore_index = True )\n",
    "except:\n",
    "    print(df_lst)\n",
    "    df = df_lst[0]\n",
    "df = df.drop(columns=['alexa_info'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns to shorter meaningful names; columns I plan to drop are not renamed\n",
    "cols = df.columns\n",
    "cols = np.array(cols)\n",
    "df = df.rename(columns = {cols[0] : 'requestID', cols[1]:'status_code', cols[2]:'res_status_xmlns', cols[3] : 'adult_content', \n",
    "                         cols[4] : 'url_text', cols[5] : 'url_type', cols[6] : 'language', cols[7] : 'lang_encoding', \n",
    "                          cols[8]:'lang_locale', cols[9]:'links_in_count', cols[11]:'owned_domains', \n",
    "                          cols[14] : 'description',cols[15]:'online_since', cols[16]:'title',cols[17] : 'med_load_time',\n",
    "                         cols[18] : 'load_speed_percentile', cols[19] : 'category_data', cols[20]:'category_path',\n",
    "                          cols[21]: 'category', cols[24]:'subdoms', cols[28]:'contr_subdom'\n",
    "                          , cols[36]:'rank',cols[38]:'rank_by_country', cols[44]:'use_stats', cols[48]:'months'})#, \n",
    "                          #cols[66]: 'alexa_rank',cols[67]:'indicator',cols[68]:'success'})\n",
    "df_full = df\n",
    "df.head(2).T\n",
    "\n",
    "#cols[20],cols[21],cols[22],cols[25?],cols[27?,28?],cols[30,31,33,36,37,38,39,40,41,42,43,44,45,46,47,] can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unneeded columns\n",
    "cols = df.columns\n",
    "vals = [0,1,3,4,5,7,8,9,11,14,15,16,17,18,19,20,21,24,28,36,38,44,67,68]\n",
    "df2 = df\n",
    "for i in range(70):\n",
    "    if i in vals:\n",
    "        print(str(i)+' passed') #just shows progress during runtime\n",
    "    else:\n",
    "        df2 = df2.drop(columns=[df.columns[i]])\n",
    "\n",
    "#df2 = df[[cols[0],cols[1],cols[2],cols[3],cols[4],cols[5],cols[7],cols[8],cols[9],cols[11],\n",
    " #         cols[14],cols[15],cols[16],cols[18],cols[19],cols[20],cols[21],cols[24],cols[28],cols[36],cols[38],cols[44],cols[66],cols[67],cols[68]]]\n",
    "df2 = df2.drop(columns=['alexa_info.html.body.center.h1','alexa_info.html.head.title'])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves simple elements of dataframe, but tosses a fair amount\n",
    "df=df2\n",
    "df['links_in_count'] = pd.to_numeric(df['links_in_count'])\n",
    "df['load_speed_percentile'] = pd.to_numeric(df['load_speed_percentile'])\n",
    "df['med_load_time'] = pd.to_numeric(df['med_load_time'])\n",
    "df['rank'] = pd.to_numeric(df['rank'])\n",
    "df['online_since'] = pd.to_datetime(df['online_since'],errors = 'coerce',yearfirst=True)\n",
    "#df['alexa_rank'] = pd.to_numeric(df['alexa_rank'],errors='coerce')\n",
    "df.drop(columns = ['owned_domains','category_data','subdoms','contr_subdom','rank_by_country','use_stats']).to_parquet('updated_main_df2.gzip',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create binary df\n",
    "df_bin = df2.copy()\n",
    "for c in df_bin.columns[1:]:\n",
    "    df_bin[c] = df_bin[c].notna()\n",
    "df_bin = df_bin.replace({True:1,False:0})\n",
    "df_bin.to_parquet('ft_presence_df.gzip',compression='gzip')\n",
    "df_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the usage statistics data\n",
    "temp = df[['indicator','use_stats']]\n",
    "\n",
    " #each item in use data is an array with 4 dictionaries that have more dicts inside them\n",
    "temp = temp.dropna().reset_index(drop=True)\n",
    "use_data = temp['use_stats']\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function can definitley be written much better than it is here\n",
    "def parse_use_data(n,ind):\n",
    "\n",
    "    udf = pd.DataFrame(use_data[n])\n",
    "    \n",
    "    lstpv = []\n",
    "    lstpv_delta = []\n",
    "    lstpvpu = []\n",
    "    lstpvpu_delta = []\n",
    "    \n",
    "    lstpv_rank = []\n",
    "    lstpv_rank_delta = []\n",
    "    \n",
    "    lstra = []\n",
    "    lstra_delta = []\n",
    "    lstre = []\n",
    "    lstre_delta = []\n",
    "    \n",
    "    lstre_rank = []\n",
    "    lstre_rank_delta = []\n",
    "    \n",
    "    lsttime = []\n",
    "    lstind = []\n",
    "    \n",
    "    for i in range(len(udf['aws:PageViews'])):\n",
    "        d = udf['aws:PageViews'][i].get('aws:PerMillion')\n",
    "        v = d.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstpv.append(np.nan)\n",
    "            lstpv_delta.append(np.nan) \n",
    "        else:\n",
    "            lstpv.append((float(v.replace(',',''))))\n",
    "            v = d.get('aws:Delta')\n",
    "            if v is None or v ==' ':\n",
    "                lstra_delta.append(np.nan)\n",
    "            else:\n",
    "                try:\n",
    "                    lstpv_delta.append(float(v.replace('%','').replace('+',''))) # need to switch from percentage string to float\n",
    "                except:\n",
    "                    lstpv_delta.append(np.nan)\n",
    "                    \n",
    "        e = udf['aws:Rank'][i]\n",
    "        v = e.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstra.append(np.nan)\n",
    "            lstra_delta.append(np.nan) \n",
    "        else:\n",
    "            lstra.append(float(v.replace(',','')))\n",
    "            v = e.get('aws:Delta')\n",
    "            if v is None:\n",
    "                lstra_delta.append(np.nan)\n",
    "            else:\n",
    "                try:\n",
    "                    lstra_delta.append(float(v)) # not a percentage \n",
    "                except:\n",
    "                    lstra_delta.append(np.nan)\n",
    "                    \n",
    "        f = udf['aws:Reach'][i].get('aws:PerMillion')\n",
    "        v = f.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstre.append(np.nan)\n",
    "            lstre_delta.append(np.nan) \n",
    "        else:\n",
    "            lstre.append(float(v.replace(',','')))\n",
    "            v = f.get('aws:Delta')\n",
    "            if v is None:\n",
    "                lstre_delta.append(np.nan) # need to switch from percentage string\n",
    "            else:\n",
    "                try:\n",
    "                    lstre_delta.append(float(v.replace('%','').replace('+','')))\n",
    "                except:\n",
    "                    lstre_delta.append(np.nan)\n",
    "        #standardizing a month as 30 days for time_range\n",
    "        g = udf['aws:TimeRange'][i]\n",
    "        gm = g.get('aws:Months')\n",
    "        gd = g.get('aws:Days')\n",
    "        val = 0\n",
    "        if gm != None:\n",
    "            val+= int(gm)*30\n",
    "        if gd != None: \n",
    "            val+= int(gd)\n",
    "        lsttime.append(val)\n",
    "        lstind.append(ind)\n",
    "        \n",
    "        h = udf['aws:PageViews'][i].get('aws:PerUser')\n",
    "        v = h.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstpvpu.append(np.nan)\n",
    "            lstpvpu_delta.append(np.nan)\n",
    "        else:\n",
    "            lstpvpu.append(float(v.replace(',','')))\n",
    "            v = h.get('aws:Delta')\n",
    "            if v is None:\n",
    "                lstpvpu_delta.append(np.nan) # need to switch from percentage string\n",
    "            else:\n",
    "                try:\n",
    "                    lstpvpu_delta.append(float(v.replace('%','').replace('+','')))\n",
    "                except:\n",
    "                    lstpvpu_delta.append(np.nan)\n",
    "        \n",
    "        k = udf['aws:PageViews'][i].get('aws:Rank')\n",
    "        v = k.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstpv_rank.append(np.nan)\n",
    "            lstpv_rank_delta.append(np.nan)\n",
    "        else:\n",
    "            lstpv_rank.append(float(v.replace(',','')))\n",
    "            v = k.get('aws:Delta')\n",
    "            if v is None:\n",
    "                lstpv_rank_delta.append(np.nan) # need to switch from percentage string\n",
    "            else:\n",
    "                try:\n",
    "                    lstpv_rank_delta.append(float(v.replace('+','')))\n",
    "                except:\n",
    "                    lstpv_rank_delta.append(np.nan)\n",
    "                    \n",
    "        l = udf['aws:PageViews'][i].get('aws:PerUser')\n",
    "        v = l.get('aws:Value')\n",
    "        if v is None: \n",
    "            lstre_rank.append(np.nan)\n",
    "            lstre_rank_delta.append(np.nan)\n",
    "        else:\n",
    "            lstre_rank.append(float(v.replace(',','')))\n",
    "            v = l.get('aws:Delta')\n",
    "            if v is None:\n",
    "                lstre_rank_delta.append(np.nan) # need to switch from percentage string\n",
    "            else:\n",
    "                try:\n",
    "                    lstre_rank_delta.append(float(v.replace('+','')))\n",
    "                except:\n",
    "                    lstre_rank_delta.append(np.nan)\n",
    "        \n",
    "    udf['indicator'] = lstind\n",
    "    udf['pg_views_per_mil'] = lstpv\n",
    "    udf['pg_views_per_mil_%change'] = lstpv_delta\n",
    "    udf['pg_views_per_user'] = lstpvpu\n",
    "    udf['pg_views_per_user_%change'] = lstpvpu_delta\n",
    "    udf['pg_views_rank'] = lstpv_rank\n",
    "    udf['pg_views_rank_change'] = lstpv_rank_delta\n",
    "    udf['rank'] = lstra\n",
    "    udf['rank_change'] = lstra_delta\n",
    "    udf['reach_per_mil'] = lstre\n",
    "    udf['reach_per_mil_%change'] = lstre_delta\n",
    "    udf['reach_rank'] = lstre_rank\n",
    "    udf['reach_rank_change'] = lstre_rank_delta\n",
    "    udf['time_range_days'] = lsttime\n",
    "    udf = udf[['indicator','pg_views_per_mil','pg_views_per_mil_%change', 'pg_views_per_user','pg_views_per_user_%change', \n",
    "               'pg_views_rank','pg_views_rank_change','rank','rank_change','reach_per_mil','reach_per_mil_%change',\n",
    "               'reach_rank','reach_rank_change','time_range_days']]\n",
    "    return udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dfs = []\n",
    "for i in range(len(temp['indicator'])):\n",
    "    res_dfs.append(parse_use_data(i,temp['indicator'][i]))\n",
    "    #print(i) #uncomment to print out index so progress is visible\n",
    "usage_data_df = pd.concat(res_dfs, axis = 0, join = 'outer', ignore_index = True)\n",
    "#usage_data_df = usage_data_df.merge(labels)\n",
    "usage_data_df.to_parquet('usage_df.gzip',compression = 'gzip')\n",
    "usage_data_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Rank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['indicator','rank_by_country']]\n",
    "temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "country_data = temp['rank_by_country']\n",
    "country_data[0]\n",
    "cdf = pd.DataFrame(country_data[0])\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['indicator','rank_by_country']]\n",
    "temp = temp.dropna().reset_index(drop=True)\n",
    "\n",
    "country_data = temp['rank_by_country']\n",
    "def parse_country_data(n,ind):\n",
    "    cdf = pd.DataFrame(country_data[n])\n",
    "    \n",
    "    lst_pv = []\n",
    "    lst_us = []\n",
    "    lst_ind = []\n",
    "    for i in range(len(cdf['Code'])):\n",
    "        v = cdf['aws:Contribution'][i].get('aws:PageViews')\n",
    "        if v is None : \n",
    "            lst_pv.append(np.nan)\n",
    "        else:\n",
    "            lst_pv.append(float((cdf['aws:Contribution'][i].get('aws:PageViews')).replace('%',''))/100)\n",
    "            \n",
    "        v = cdf['aws:Contribution'][i].get('aws:Users')\n",
    "        if v is None:\n",
    "            lst_us.append(np.nan)\n",
    "        else:\n",
    "            lst_us.append(float((cdf['aws:Contribution'][i].get('aws:Users')).replace('%',''))/100)\n",
    "        lst_ind.append(ind)\n",
    "    cdf['indicator'] = lst_ind\n",
    "    cdf['pg_view_contribution'] = lst_pv\n",
    "    cdf['user_contribution'] = lst_us\n",
    "    cdf = cdf.rename(columns={'Code':'country_code', 'aws:Contribution': 'contribution', 'aws:Rank':'rank'})\n",
    "    cdf = cdf[['indicator','country_code','rank', 'pg_view_contribution','user_contribution']]\n",
    "    return cdf\n",
    "\n",
    "res_dfs = []\n",
    "for i in range(len(temp['indicator'])):\n",
    "    res_dfs.append(parse_country_data(i,temp['indicator'][i]))\n",
    "    print(i)\n",
    "country_rank_df = pd.concat(res_dfs, axis = 0, join = 'outer', ignore_index = True)\n",
    "country_rank_df = country_rank_df.merge(labels)\n",
    "country_rank_df.to_parquet('country_rank_df.gzip',compression = 'gzip')\n",
    "country_rank_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owned Domains Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df[['indicator','owned_domains']].dropna().reset_index(drop=True)\n",
    "lst_ind = []\n",
    "lst_dom = []\n",
    "lst_title = [] \n",
    "\n",
    "for i in range(len(ddf['indicator'])):\n",
    "    d = ddf['owned_domains'][i]\n",
    "    for x in d:\n",
    "        lst_ind.append(ddf['indicator'][i])\n",
    "        lst_dom.append(x.get('aws:Domain'))\n",
    "        lst_title.append(x.get('aws:Title'))\n",
    "domain_df = pd.DataFrame({'indicator':lst_ind, 'domain_name':lst_dom,'domain_title':lst_title})\n",
    "domain_df = domain_df[['indicator','domain_name','domain_title']]\n",
    "domain_df.to_parquet('owned_domain_df.gzip',compression='gzip')\n",
    "domain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing Subdomain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sddf = df[['indicator','contr_subdom']].dropna().reset_index(drop=True)\n",
    "lst_ind = []\n",
    "lst_url = []\n",
    "lst_pvpu = []\n",
    "lst_pvperc = []\n",
    "lst_reperc = []\n",
    "lst_time = []\n",
    "\n",
    "for i in range(len(sddf['indicator'])):\n",
    "    ind = sddf['indicator'][i]\n",
    "    for x in sddf['contr_subdom'][i]:\n",
    "        lst_ind.append(ind)\n",
    "        lst_url.append(x.get('aws:DataUrl'))\n",
    "        lst_pvpu.append(float(x.get('aws:PageViews').get('aws:PerUser')))\n",
    "        lst_pvperc.append(float(x.get('aws:PageViews').get('aws:Percentage').replace('%','')))\n",
    "        lst_reperc.append(float(x.get('aws:Reach').get('aws:Percentage').replace('%','')))\n",
    "        days = 0\n",
    "        try:\n",
    "            days += int(x.get('aws:TimeRange').get('aws:Months'))*30\n",
    "        except:\n",
    "            days += 0\n",
    "        try:\n",
    "            days += int(x.get('aws:TimeRange').get('aws:Days'))\n",
    "        except: \n",
    "            days += 0\n",
    "        lst_time.append(days)\n",
    "contr_subdom_df = pd.DataFrame({'indicator':lst_ind, 'data_url':lst_url, 'page_views_per_user':lst_pvpu, \n",
    "                                'page_views_%':lst_pvperc, 'reach_%':lst_reperc, 'time_range_days': lst_time})\n",
    "contr_subdom_df.to_parquet('contributing_subdomain_df.gzip',compression='gzip')\n",
    "contr_subdom_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catdf = df[['indicator','category_data']].dropna().reset_index(drop=True)\n",
    "lst_ind = []\n",
    "lst_title=[]\n",
    "lst_path = []\n",
    "for i in range(len(catdf['indicator'])):\n",
    "    ind = catdf['indicator'][i]\n",
    "    for x in catdf['category_data'][i]:\n",
    "        lst_ind.append(ind)\n",
    "        lst_title.append(x.get('aws:Title'))\n",
    "        lst_path.append(x.get('aws:AbsolutePath'))\n",
    "category_df = pd.DataFrame({'indicator':lst_ind, 'path':lst_path,'title':lst_title})\n",
    "category_df.to_parquet('category_df.gzip',compression = 'gzip')\n",
    "category_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
