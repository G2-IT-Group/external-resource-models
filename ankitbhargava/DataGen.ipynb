{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the \"Subnet Identifier\" notebook first, then this code in its entirety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traceroute Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook containing preliminary exploration of cybersecuirty related data pulled from both benign and malicious sources\n",
    "# with the goal of identifying suspicious websites using various machine learning models.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pprint\n",
    "import operator\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from ipwhois import IPWhois\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from bisect import bisect_left\n",
    "import socket, struct\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate filenames of all json documents for stragithforward iteration\n",
    "\n",
    "date_paths = '/data/data/'\n",
    "everyFile=os.listdir(date_paths)\n",
    "everyFile=['/data/data/'+x+'/enrichment/traceroute/' for x in everyFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "everyFile=everyFile[31:]\n",
    "everyFile.remove('/data/data/2019-04-13/enrichment/traceroute/')\n",
    "everyFile.remove('/data/data/2019-05-13/enrichment/traceroute/')\n",
    "everyFile.remove('/data/data/2019-05-14/enrichment/traceroute/')\n",
    "everyFile.remove('/data/data/canyouseethisfile.txt/enrichment/traceroute/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempEvery=[]\n",
    "for x in everyFile:\n",
    "    for s in os.listdir(x):\n",
    "        if (len(s) > 10):\n",
    "            tempEvery.append(x+s)\n",
    "\n",
    "for s in os.listdir('/data/data/newbenigndata/enrichment/traceroute/'):\n",
    "        tempEvery.append('/data/data/newbenigndata/enrichment/traceroute/'+s)\n",
    "        \n",
    "for s in os.listdir('/data/data/newNONTORbenigndata/enrichment/traceroute/'):\n",
    "        tempEvery.append('/data/data/newNONTORbenigndata/enrichment/traceroute/'+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13501 total traceroute files\n"
     ]
    }
   ],
   "source": [
    "print(len(tempEvery), \"total traceroute files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode IPs by Subnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2874\n"
     ]
    }
   ],
   "source": [
    "# Json containing all subnets in the dataset\n",
    "consolidated=json.load(open('SerializedResources/reducedConsolidated.json'))\n",
    "k=list(consolidated.keys())\n",
    "k=sorted(k)\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCIDR(alpha):\n",
    "    ip=alpha[:alpha.find('/')]\n",
    "    mask=alpha[alpha.find('/')+1:]\n",
    "    return (ip, int(mask))\n",
    "    \n",
    "def dottedQuadToNum(ip):\n",
    "    \"convert decimal dotted quad string to long integer\"\n",
    "    return struct.unpack('!L',socket.inet_aton(ip))[0]\n",
    "\n",
    "def numToDottedQuad(n):\n",
    "    \"convert long int to dotted quad string\"\n",
    "    return socket.inet_ntoa(struct.pack('!L',n))\n",
    "      \n",
    "def makeMask(n):\n",
    "    \"return a mask of n bits as a long integer\"\n",
    "    return (1 << 32-n)-1\n",
    "\n",
    "def ipToNetAndMask(ip):\n",
    "    \"returns tuple (network, host) dotted-quad addresses given IP and mask size\"\n",
    "    if (len(ip) > 18):\n",
    "        ip=ip[:ip.find(',')]\n",
    "    network,mask = parseCIDR(ip)\n",
    "    n = dottedQuadToNum(network)\n",
    "    m = makeMask(mask)\n",
    "\n",
    "    host = n & m\n",
    "    net = n - host\n",
    "\n",
    "    return numToDottedQuad(net), mask\n",
    "\n",
    "def toNet(network,maskbits):\n",
    "    \"returns tuple (network, host) dotted-quad addresses given IP and mask size\"\n",
    "    n = dottedQuadToNum(network)\n",
    "    m = makeMask(maskbits)\n",
    "\n",
    "    host = n & m\n",
    "    net = n - host\n",
    "\n",
    "    return numToDottedQuad(net)\n",
    "\n",
    "\n",
    "unique_ips=set()\n",
    "def oneHotEncode(hops):\n",
    "    unregistered=0\n",
    "    #global unique_ips\n",
    "    ret={}\n",
    "    order=0\n",
    "    for point in hops:\n",
    "        #unique_ips.add(point)\n",
    "        if (point != '***'):\n",
    "            for i in range (32,0,-1):\n",
    "                ref=toNet(point,i)\n",
    "                if (consolidated.get(ref,False)):\n",
    "                    if (consolidated[ref] == i):\n",
    "                        ret[order]=bisect_left(k,ref)\n",
    "                        break\n",
    "\n",
    "            \n",
    "        order+=1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "1.0%\n",
      "2.0%\n",
      "3.0%\n",
      "4.0%\n",
      "5.0%\n",
      "6.0%\n",
      "7.0%\n",
      "8.0%\n",
      "9.0%\n",
      "10.0%\n",
      "11.0%\n",
      "12.0%\n",
      "13.0%\n",
      "14.0%\n",
      "15.0%\n",
      "16.0%\n",
      "17.0%\n",
      "18.0%\n",
      "19.0%\n",
      "20.0%\n",
      "21.0%\n",
      "22.0%\n",
      "23.0%\n",
      "24.0%\n",
      "25.0%\n",
      "26.0%\n",
      "27.0%\n",
      "28.0%\n",
      "29.0%\n",
      "30.0%\n",
      "31.0%\n",
      "32.0%\n",
      "33.0%\n",
      "34.0%\n",
      "35.0%\n",
      "36.0%\n",
      "37.0%\n",
      "38.0%\n",
      "39.0%\n",
      "40.0%\n",
      "41.0%\n",
      "42.0%\n",
      "43.0%\n",
      "44.0%\n",
      "45.0%\n",
      "46.0%\n",
      "47.0%\n",
      "48.0%\n",
      "49.0%\n",
      "50.0%\n",
      "51.0%\n",
      "52.0%\n",
      "53.0%\n",
      "54.0%\n",
      "55.0%\n",
      "56.0%\n",
      "57.0%\n",
      "58.0%\n",
      "59.0%\n",
      "60.0%\n",
      "61.0%\n",
      "62.0%\n",
      "63.0%\n",
      "64.0%\n",
      "65.0%\n",
      "66.0%\n",
      "67.0%\n",
      "68.0%\n",
      "69.0%\n",
      "70.0%\n",
      "71.0%\n",
      "72.0%\n",
      "73.0%\n",
      "74.0%\n",
      "75.0%\n",
      "76.0%\n",
      "77.0%\n",
      "78.0%\n",
      "79.0%\n",
      "80.0%\n",
      "81.0%\n",
      "82.0%\n",
      "83.0%\n",
      "84.0%\n",
      "85.0%\n",
      "86.0%\n",
      "87.0%\n",
      "88.0%\n",
      "89.0%\n",
      "90.0%\n",
      "91.0%\n",
      "92.0%\n",
      "93.0%\n",
      "94.0%\n",
      "95.0%\n",
      "96.0%\n",
      "97.0%\n",
      "98.0%\n",
      "99.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "frames=[]\n",
    "expanded_route=[]\n",
    "numEmpties=0\n",
    "s=-1\n",
    "for w in tempEvery:\n",
    "    s+=1\n",
    "    #w=tempEvery[random.randint(0,len(tempEvery))]\n",
    "    #tempEvery.remove(w)\n",
    "    if (s%135 == 0):\n",
    "        print(str(s/135)+'%')\n",
    "\n",
    "    try:\n",
    "        temp_json=json.load(open(w))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    if type(temp_json[\"data\"]) == list and len(temp_json[\"data\"]) > 0:\n",
    "        frames.append(pd.DataFrame(temp_json[\"data\"]))  \n",
    "    else:\n",
    "        continue\n",
    "    #For each element in the data list from the traceroute json...\n",
    "    #all_trace=[]\n",
    "    #all_ping=[]\n",
    "    all_benign=[]\n",
    "    all_dest=[]\n",
    "    all_route_lengths=[]\n",
    "    all_avg_ping=[]\n",
    "    all_timeouts=[]\n",
    "    #all_weighted_ping=[]\n",
    "    all_tailTimes=[]\n",
    "    all_reached=[]\n",
    "    #expanded_route=[]\n",
    "    \n",
    "    for route in temp_json[\"data\"]:\n",
    "        populated=True\n",
    "        #Error handling\n",
    "        if type(route) is str:\n",
    "            populated=False\n",
    "            break\n",
    "            \n",
    "        # Determine benign or malicious feature set\n",
    "        if (len(w)-w.rfind('/')<10):\n",
    "            all_benign.append(True)\n",
    "        else:\n",
    "            all_benign.append(False)\n",
    "            \n",
    "        #Parse the string of traceroute data\n",
    "        split=route[\"traceroute\"].splitlines()\n",
    "        all_route_lengths.append(len(split)-1)\n",
    "        hops=[]\n",
    "        pings=[]\n",
    "        \n",
    "        #For each line in the traceroute for a given indicator\n",
    "        count=0\n",
    "        timeouts=0\n",
    "        for x in split:\n",
    "            regIP=re.findall(\"([(]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[)])\",x)\n",
    "            regPing=re.findall(\"\\s\\d+[.]\\d{3}\\s\",x)\n",
    "            pings.append(regPing)\n",
    "            for ip in regIP:\n",
    "                if (count ==0):\n",
    "                    all_dest.append(ip[1:len(ip)-1])\n",
    "                    count=1\n",
    "                ip=ip[1:-1]\n",
    "                hops.append(ip)\n",
    "                break\n",
    "               \n",
    "            if (count == 0):\n",
    "                all_dest.append(\"***\")\n",
    "                count=1\n",
    "                \n",
    "            if (len(regIP) == 0):\n",
    "                hops.append(\"***\")\n",
    "                timeouts=timeouts+1\n",
    "            \n",
    "        tailTimes=0\n",
    "        \n",
    "        for x in hops[::-1]:\n",
    "            if (x == '***'):\n",
    "                tailTimes += 1\n",
    "            else:\n",
    "                all_reached.append(x == all_dest[len(all_dest)-1])\n",
    "                break\n",
    "        else:\n",
    "            all_reached.append(False)\n",
    "             \n",
    "        all_tailTimes.append(tailTimes)\n",
    "        # One-hot encoding\n",
    "        #t0=time.time()\n",
    "        #newRow=\n",
    "        #t1=time.time()\n",
    "        expanded_route.append(oneHotEncode(hops))\n",
    "        #print((t1-t0),(time.time()-t1))\n",
    "        \n",
    "        #all_ping.append(pings[1:])\n",
    "        all_timeouts.append(timeouts)\n",
    "        overallPing=0\n",
    "        #weighted_ping=0\n",
    "        idx=1\n",
    "        for trio in pings:\n",
    "            if (len(trio) != 0):\n",
    "                overallPing=overallPing+float(min(trio))\n",
    "                #weighted_ping=weighted_ping+float(min(trio))*idx*idx\n",
    "                idx+=1\n",
    "                \n",
    "        try:\n",
    "            all_avg_ping.append(overallPing/(all_route_lengths[len(all_route_lengths)-1]-timeouts))\n",
    "        except:\n",
    "            all_avg_ping.append(0)\n",
    "            \n",
    "        #try:\n",
    "        #    all_weighted_ping.append(weighted_ping/(all_route_lengths[len(all_route_lengths)-1]-timeouts))\n",
    "        #except:\n",
    "        #    all_weighted_ping.append(0)\n",
    "    #print(frames[len(frames)-1].shape)\n",
    "    #print(len(all_trace))\n",
    "    #print(w)\n",
    "    \n",
    "   \n",
    "    if (populated):\n",
    "        frames[len(frames)-1]=frames[len(frames)-1].drop(columns=['traceroute','success'])\n",
    "        frames[len(frames)-1].insert(1,\"Reached\",all_reached)\n",
    "        frames[len(frames)-1].insert(1,\"Benign\",all_benign)\n",
    "        frames[len(frames)-1].insert(2,\"Dest\",all_dest)\n",
    "        frames[len(frames)-1].insert(2,\"NumHops\",all_route_lengths)\n",
    "        frames[len(frames)-1].insert(3,\"Tail Timeouts\",all_tailTimes)\n",
    "        frames[len(frames)-1].insert(2,\"AveragePing\",all_avg_ping)\n",
    "        frames[len(frames)-1].insert(2,\"Timeouts\",all_timeouts)\n",
    "        #frames[len(frames)-1]=pd.concat([frames[len(frames)-1], pd.DataFrame(expanded_route, columns=[str(i) for i in range(len(expanded_route[0]))])], axis=1)\n",
    "        #frames[len(frames)-1]=pd.concat([frames[len(frames)-1],pd.concat(expanded_route)],axis=1,join_axes=[frames[len(frames)-1].index])\n",
    "        #frames[len(frames)-1]\n",
    "    else:\n",
    "        populated=True\n",
    "        frames.pop()\n",
    "\n",
    "#del all_trace\n",
    "#del all_ping\n",
    "del all_benign\n",
    "del all_dest\n",
    "del all_route_lengths\n",
    "del all_avg_ping\n",
    "del all_timeouts\n",
    "#del all_weighted_ping\n",
    "del pings\n",
    "del hops\n",
    "#del expanded_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(frames)\n",
    "del frames\n",
    "df=df.fillna(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (515253, 8)\n",
      "Benign 23975\n",
      "Malicious 491278\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions',df.shape)\n",
    "print('Benign', len(df[df.Benign== True]))\n",
    "print('Malicious', len(df[df.Benign== False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spare regex expressions \n",
    "\n",
    "\n",
    "#print(time.time()-t1)           \n",
    "\n",
    "#regDNS=re.findall(\"(\\s[\\w\\-._~:\\/\\?\\#\\[\\]\\@\\!\\$\\&\\'\\(\\)\\*\\+\\,\\;\\=]+\\s)([(]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[)])\",x)\n",
    "#reg1=re.findall(\"([(]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[)])(\\s+\\d+[.]\\d{3}\\sms)+\",x)\n",
    "#reg1=re.findall(\"[(]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[)]\\s+\\d+[.]\\d{3}\\s\",x)\n",
    "\n",
    "#q=-1\n",
    "#for x in s:\n",
    "    #q=q+1\n",
    "    #Parse ip adresses and latency\n",
    "    #print(q, re.findall(\"[(]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[.]\\d{1,3}[)]\",x), re.findall(\"\\s\\d+[.]\\d{3}\\s\",x))\n",
    "    #print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=df.reset_index(drop=True)\n",
    "df.to_parquet(\"BigExtractedFeatures.parquet\")\n",
    "\n",
    "with open('BigExpanded.pickle', 'wb') as output:  # Overwrites any existing file.\n",
    "    pickle.dump(expanded_route, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
