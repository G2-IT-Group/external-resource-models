{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and Sequential Modeling of Traceroutes\n",
    "\n",
    "Treating the traceroute as a sequence of steps towards a destination could reveal more nuanced patterns. Accuracy with these models can be as high as 80% but the hypothesis that additional information may be encoded in the order was disproven as the LSTM had no additional value when compared with the standard neural nets.  Therefore, the only thing that matters is which subnets are used, not necessarily in which order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import socket, struct\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, backend, models\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13837181204682714592\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11284293223\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 302216122573034960\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('BigFtaglOrdered.pickle', 'rb') \n",
    "ftagl=pickle.load(f)\n",
    "f= open('BigExpanded.pickle', 'rb') \n",
    "expanded_routes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data structures (skip if serializied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.zeros(shape=[len(expanded_routes),30,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Transform raw route information into LSTM structure with dimensionality reduction\n",
    "## Note: Feature Agglomeration was done on all days\n",
    "\n",
    "row=0\n",
    "numEmpty=0\n",
    "df_rows=np.zeros(shape=(len(expanded_routes),2874))\n",
    "for seq in expanded_routes:\n",
    "    vec=np.zeros(shape=[30,2874])\n",
    "    df_vec=np.zeros(shape=[1,2874])\n",
    "    there=0\n",
    "    for i in range(30):\n",
    "        if seq.get(i):\n",
    "            vec[i][seq[i]]=1\n",
    "            df_vec[0][seq[i]]=1\n",
    "            there+=1\n",
    "\n",
    "            \n",
    "    vec=ftagl.transform(vec)\n",
    "    df_rows[row]=df_vec\n",
    "    X[row]=vec\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftagl.fit(df_rows)\n",
    "#with open('BigFtaglOrdered.pickle', 'wb') as output:\n",
    "#    pickle.dump(ftagl,output,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df_rows)\n",
    "del df_rows\n",
    "del expanded_routes\n",
    "df=pd.concat([pd.read_parquet('BigExtractedFeatures.parquet'),df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[str(x) for x in df.columns]\n",
    "df.to_parquet('universe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('X',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load serialized data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df[df['NumHops']>2]\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction if desired\n",
    "\n",
    "# Fit new dim reduction matrix, not necessary if *ftagl*.pickle exists\n",
    "#ftagl = cluster.FeatureAgglomeration(n_clusters=100)\n",
    "\n",
    "reduced = ftagl.transform(df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process  and balance data\n",
    "\n",
    "#df=df.drop(columns=df.columns.difference([header for header in df.columns if not (str(header).isdigit())]))\n",
    "#reduced=pd.DataFrame(reduced)\n",
    "#df=pd.concat([df,reduced],axis=1)\n",
    "#del reduced\n",
    "\n",
    "\n",
    "bd=df[df['Benign'] == True]\n",
    "even=len(bd)\n",
    "md=df[df['Benign'] == False]\n",
    "md.reset_index(drop=True)\n",
    "\n",
    "bd=bd.sample(frac=1).reset_index(drop=True)\n",
    "md=md.sample(frac=1).reset_index(drop=True)\n",
    "md=md.loc[md.index < even]\n",
    "ad=pd.concat([bd,md])\n",
    "\n",
    "del bd\n",
    "del md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22255 22255\n"
     ]
    }
   ],
   "source": [
    "print(len(ad[ad['Benign']==True]),len(ad[ad['Benign']==False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=ad.columns.difference(['indicator','Benign','Dest','Route','index'])\n",
    "ad=ad.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_dim=2879),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a sigmoid layer for outputs:\n",
    "layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35608 samples, validate on 8902 samples\n",
      "Epoch 1/5\n",
      "35608/35608 [==============================] - 42s 1ms/step - loss: 0.5235 - acc: 0.7425 - val_loss: 0.4550 - val_acc: 0.7852\n",
      "Epoch 2/5\n",
      "35608/35608 [==============================] - 39s 1ms/step - loss: 0.4325 - acc: 0.7958 - val_loss: 0.4217 - val_acc: 0.8011\n",
      "Epoch 3/5\n",
      "35608/35608 [==============================] - 40s 1ms/step - loss: 0.4018 - acc: 0.8113 - val_loss: 0.4085 - val_acc: 0.8081\n",
      "Epoch 4/5\n",
      "35608/35608 [==============================] - 40s 1ms/step - loss: 0.3907 - acc: 0.8175 - val_loss: 0.4061 - val_acc: 0.8086\n",
      "Epoch 5/5\n",
      "35608/35608 [==============================] - 40s 1ms/step - loss: 0.3781 - acc: 0.8241 - val_loss: 0.4117 - val_acc: 0.8033\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#cease=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)\n",
    "history=model.fit(ad[exclude],ad['Benign'], epochs=5, verbose=1, batch_size=64,\n",
    "          validation_split=.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.86      0.83     22255\n",
      "        True       0.85      0.79      0.82     22255\n",
      "\n",
      "    accuracy                           0.83     44510\n",
      "   macro avg       0.83      0.83      0.83     44510\n",
      "weighted avg       0.83      0.83      0.83     44510\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[17661  4594]\n",
      " [ 3128 19127]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(ad['Benign'], model.predict(ad[exclude])>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(ad['Benign'], model.predict(ad[exclude]) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LSTM \n",
    "\n",
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data, labels, dimReduction, batch_size=64, dim=(30,50), n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.expanded_routes=data\n",
    "        self.benign_label=labels\n",
    "        self.index=0\n",
    "        self.dimReduction=dimReduction\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return int(np.floor(len(self.expanded_routes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self,ignore):\n",
    "\n",
    "        X=np.zeros(shape=[self.batch_size,self.dim[0],self.dim[1]])\n",
    "        y=np.array(self.benign_label[self.index:self.index+self.batch_size])\n",
    "        \n",
    "        #row=0\n",
    "        #while row < batch_size:\n",
    "        #for s in range(self.index,self.index+self.batch_size):\n",
    "           \n",
    "        #    for i in range(30):\n",
    "        #        vec=np.zeros(shape=[1,7566])\n",
    "        #        if self.expanded_routes[s].get(i):\n",
    "        #            vec[0][self.expanded_routes[s][i]]=1\n",
    "       #        \n",
    "       #         vec=self.dimReduction.transform(vec)\n",
    "       #         X[row][i]=vec\n",
    "       #         \n",
    "       #     row+=1\n",
    "\n",
    "       # self.index=self.index+self.batch_size\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        if self.shuffle == True:\n",
    "            print('Shuffled')\n",
    "        \n",
    "        self.index=0\n",
    "\n",
    "    def __data_generation(self,rand=None):\n",
    "\n",
    "        # Initialization\n",
    "        X=np.random.rand(self.batch_size,30,50)\n",
    "        y = np.round(np.random.rand((self.batch_size)))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "keep=df['NumHops']>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515253, 30, 50) (515253,) (515253, 5)\n"
     ]
    }
   ],
   "source": [
    "## Define empty training vars to be populated later\n",
    "\n",
    "X=np.load('X.npz')['arr_0']\n",
    "X=X[keep]\n",
    "Y=np.array(df['Benign'])\n",
    "Y=Y[keep]\n",
    "X_e=np.array(df[['Timeouts','AveragePing','NumHops','Tail Timeouts','Reached']])\n",
    "X_e=X_e[keep]\n",
    "df=df[keep]\n",
    "df=df.reset_index(drop=True)\n",
    "print(X.shape, Y.shape, X_e.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map benign and malicious entries for balancing purposes\n",
    "\n",
    "benignIndices=[]\n",
    "maliciousIndices=[]\n",
    "for x in range(0,len(df)):\n",
    "    if df['Benign'][x] == True:\n",
    "        benignIndices.append(x)\n",
    "    else:\n",
    "        maliciousIndices.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data from equal parts benign and malicious data, shuffle to avoid proximity effects\n",
    "\n",
    "X_benign_e=X_e[benignIndices,:]\n",
    "X_benign=X[benignIndices,:]\n",
    "Y_benign=Y[benignIndices]\n",
    "X_mal=X[maliciousIndices,:]\n",
    "X_mal_e=X_e[maliciousIndices,:]\n",
    "Y_mal=Y[maliciousIndices]\n",
    "del X\n",
    "del Y\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_mal)\n",
    "X_mal_e=X_mal_e[0:len(X_benign),:]\n",
    "X_mal=X_mal[0:len(X_benign),:]\n",
    "Y_mal=Y_mal[0:len(X_benign)]\n",
    "X_t_e=np.concatenate((X_benign_e,X_mal_e), axis=0)\n",
    "X_t=np.concatenate((X_benign,X_mal), axis=0)\n",
    "Y_t=np.concatenate((Y_benign,Y_mal), axis=0)\n",
    "del X_benign\n",
    "del X_benign_e\n",
    "del X_mal\n",
    "del X_mal_e\n",
    "del Y_benign\n",
    "del Y_mal\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 35,713\n",
      "Trainable params: 35,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds an LSTM layer to intake the traceroute as a sequence of stops:\n",
    "layers.LSTM(64, activation='relu', input_shape=(30,50), return_sequences=False),\n",
    "# Add dense layers to further interpret results:\n",
    "layers.Dense(64, activation='relu'),\n",
    "    \n",
    "layers.Dense(32, activation='relu'),\n",
    "# Add a sigmoid output layer\n",
    "layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35608 samples, validate on 8902 samples\n",
      "Epoch 1/10\n",
      "35608/35608 [==============================] - 15s 432us/step - loss: 0.5967 - acc: 0.6729 - val_loss: 0.5678 - val_acc: 0.6898\n",
      "Epoch 2/10\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.5611 - acc: 0.7011 - val_loss: 0.5473 - val_acc: 0.7146\n",
      "Epoch 3/10\n",
      "35608/35608 [==============================] - 15s 423us/step - loss: 0.5430 - acc: 0.7119 - val_loss: 0.5341 - val_acc: 0.7197\n",
      "Epoch 4/10\n",
      "35608/35608 [==============================] - 15s 425us/step - loss: 0.5315 - acc: 0.7224 - val_loss: 0.6685 - val_acc: 0.6921\n",
      "Epoch 5/10\n",
      "35608/35608 [==============================] - 15s 411us/step - loss: 0.5383 - acc: 0.7208 - val_loss: 0.5384 - val_acc: 0.7121\n",
      "Epoch 6/10\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.5216 - acc: 0.7305 - val_loss: 0.5217 - val_acc: 0.7352\n",
      "Epoch 7/10\n",
      "35608/35608 [==============================] - 15s 411us/step - loss: 0.5203 - acc: 0.7298 - val_loss: 0.5313 - val_acc: 0.7221\n",
      "Epoch 8/10\n",
      "35608/35608 [==============================] - 15s 410us/step - loss: 0.5128 - acc: 0.7351 - val_loss: 0.5166 - val_acc: 0.7317\n",
      "Epoch 9/10\n",
      "35608/35608 [==============================] - 15s 411us/step - loss: 0.5111 - acc: 0.7365 - val_loss: 0.5170 - val_acc: 0.7325\n",
      "Epoch 10/10\n",
      "35608/35608 [==============================] - 15s 411us/step - loss: 0.5101 - acc: 0.7362 - val_loss: 0.5119 - val_acc: 0.7320\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(X_t,Y_t, epochs=10, verbose=1, batch_size=64,\n",
    "          validation_split=0.2, shuffle=True)\n",
    "\n",
    "#training_generator = DataGenerator(expanded_routes,df['Benign'],ftagl,batch_size=2048)\n",
    "#validation_generator=DataGenerator(expanded_routes[0],df['Benign'][0],ftagl)\n",
    "#history=model.fit_generator(generator=training_generator,validation_data=validation_generator,use_multiprocessing=True,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.random.random((1000, 32))\n",
    "#labels = random_one_hot_labels((1000, 10))\n",
    "\n",
    "model.evaluate(X_t, Y_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.71      0.81      0.76     22255\n",
      "        True       0.78      0.67      0.72     22255\n",
      "\n",
      "    accuracy                           0.74     44510\n",
      "   macro avg       0.74      0.74      0.74     44510\n",
      "weighted avg       0.74      0.74      0.74     44510\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14981  7274]\n",
      " [ 4288 17967]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(Y_t, model.predict(X_t)>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_t, model.predict(X_t) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Hybrid LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44510, 5) (44510, 30, 50) (44510,)\n"
     ]
    }
   ],
   "source": [
    "print(X_t_e.shape, X_t.shape, Y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 30, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           29440       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 69)           0           lstm_1[0][0]                     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           4480        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            65          dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,145\n",
      "Trainable params: 38,145\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Two input sequences, the first is standard statistics about the traceroute and the second the sequence of stops \n",
    "my_inputs = layers.Input(shape=(5,),dtype='float32')\n",
    "ip_inputs = layers.Input(shape=(30,50),dtype='float32')\n",
    "\n",
    "# LSTM layer\n",
    "lstm_out = layers.LSTM(64, activation='relu')(ip_inputs)\n",
    "\n",
    "# Merge outputs from the LSTM layer with the next set of inputs\n",
    "x = keras.layers.concatenate([lstm_out, my_inputs])\n",
    "\n",
    "# Add dense layers to combine LSTM information with other data\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Sigmoid output layer\n",
    "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = models.Model(inputs=[my_inputs,ip_inputs], outputs=predictions)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#board=keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35608 samples, validate on 8902 samples\n",
      "Epoch 1/20\n",
      "35608/35608 [==============================] - 16s 448us/step - loss: 0.5940 - acc: 0.7054 - val_loss: 0.5259 - val_acc: 0.7357\n",
      "Epoch 2/20\n",
      "35608/35608 [==============================] - 16s 439us/step - loss: 0.5148 - acc: 0.7485 - val_loss: 0.5044 - val_acc: 0.7502\n",
      "Epoch 3/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.5003 - acc: 0.7542 - val_loss: 0.5209 - val_acc: 0.7390\n",
      "Epoch 4/20\n",
      "35608/35608 [==============================] - 15s 415us/step - loss: 0.4863 - acc: 0.7614 - val_loss: 0.4876 - val_acc: 0.7638\n",
      "Epoch 5/20\n",
      "35608/35608 [==============================] - 15s 418us/step - loss: 0.4812 - acc: 0.7637 - val_loss: 0.4748 - val_acc: 0.7711\n",
      "Epoch 6/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4766 - acc: 0.7678 - val_loss: 0.5119 - val_acc: 0.7466\n",
      "Epoch 7/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4752 - acc: 0.7676 - val_loss: 0.4692 - val_acc: 0.7717\n",
      "Epoch 8/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4700 - acc: 0.7726 - val_loss: 0.4685 - val_acc: 0.7730\n",
      "Epoch 9/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4682 - acc: 0.7728 - val_loss: 0.4684 - val_acc: 0.7739\n",
      "Epoch 10/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4646 - acc: 0.7756 - val_loss: 0.4768 - val_acc: 0.7716\n",
      "Epoch 11/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4627 - acc: 0.7769 - val_loss: 0.4642 - val_acc: 0.7761\n",
      "Epoch 12/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4606 - acc: 0.7767 - val_loss: 0.4721 - val_acc: 0.7752\n",
      "Epoch 13/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4560 - acc: 0.7789 - val_loss: 0.4608 - val_acc: 0.7778\n",
      "Epoch 14/20\n",
      "35608/35608 [==============================] - 16s 439us/step - loss: 0.4556 - acc: 0.7805 - val_loss: 0.4671 - val_acc: 0.7742\n",
      "Epoch 15/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4678 - acc: 0.7747 - val_loss: 0.4604 - val_acc: 0.7774\n",
      "Epoch 16/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4528 - acc: 0.7820 - val_loss: 0.4628 - val_acc: 0.7803\n",
      "Epoch 17/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4485 - acc: 0.7854 - val_loss: 0.4659 - val_acc: 0.7781\n",
      "Epoch 18/20\n",
      "35608/35608 [==============================] - 15s 413us/step - loss: 0.4464 - acc: 0.7873 - val_loss: 0.4610 - val_acc: 0.7795\n",
      "Epoch 19/20\n",
      "35608/35608 [==============================] - 15s 412us/step - loss: 0.4445 - acc: 0.7890 - val_loss: 0.4570 - val_acc: 0.7870\n",
      "Epoch 20/20\n",
      "35608/35608 [==============================] - 15s 414us/step - loss: 0.4424 - acc: 0.7902 - val_loss: 0.4600 - val_acc: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4eb183c908>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_t_e,X_t], Y_t, epochs=20, verbose =1, batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_t\n",
    "del Y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder \n",
    "\n",
    "#### Reduce dimensionality of subnet data\n",
    "#### Failed experiment: it appears the the data is too sparse for the sutoencoder to learn anything.  The output is always the same set of 4 or 5 entries predicted as ones regardless of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df[df['NumHops']>2]\n",
    "df=df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])]\n",
    "df=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    464614.000000\n",
       "mean          7.606760\n",
       "std           1.894357\n",
       "min           0.000000\n",
       "25%           6.000000\n",
       "50%           7.000000\n",
       "75%           9.000000\n",
       "max          16.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df.sum(axis=1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2874)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               736000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2874)              738618    \n",
      "=================================================================\n",
      "Total params: 1,474,618\n",
      "Trainable params: 1,474,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 256\n",
    "\n",
    "# input vec\n",
    "input_vec = layers.Input(shape=(2874,))\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(x)\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(encoded)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(x)\n",
    "\n",
    "# \"decoded\" is the reconstruction of the input\n",
    "decoded = layers.Dense(2874, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = models.Model(input_vec, decoded)\n",
    "autoencoder.summary()\n",
    "#encoder = models.Model(input_vec, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "#encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "#decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "#decoder = models.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 464614 samples, validate on 464614 samples\n",
      "Epoch 1/1\n",
      "464614/464614 [==============================] - 116s 249us/step - loss: 0.0035 - acc: 0.1217 - val_loss: 0.0013 - val_acc: 0.1222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f56b880b400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=tf.train.AdamOptimizer(1), loss='mse',metrics=['accuracy'])\n",
    "\n",
    "autoencoder.fit(df,df,\n",
    "                epochs=1,\n",
    "                batch_size=4096,\n",
    "                shuffle=True,\n",
    "                validation_data=(df,df),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=autoencoder.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2864</th>\n",
       "      <th>2865</th>\n",
       "      <th>2866</th>\n",
       "      <th>2867</th>\n",
       "      <th>2868</th>\n",
       "      <th>2869</th>\n",
       "      <th>2870</th>\n",
       "      <th>2871</th>\n",
       "      <th>2872</th>\n",
       "      <th>2873</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   2864  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   2865  2866  2867  2868  2869  2870  2871  2872  2873  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1 rows x 2874 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=pd.DataFrame(pred).drop_duplicates()\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
