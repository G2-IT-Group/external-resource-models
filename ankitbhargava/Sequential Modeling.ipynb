{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and Sequential Modeling of Traceroutes\n",
    "\n",
    "Treating the traceroute as a sequence of steps towards a destination could reveal more nuanced patterns. Accuracy with these models can be as high as 80% but the hypothesis that additional information may be encoded in the order was disproven as the LSTM had no additional value when compared with the standard neural nets.  Therefore, the only thing that matters is which subnets are used, not necessarily in which order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import socket, struct\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, backend, models\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator FeatureAgglomeration from version 0.21.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "f= open('BigFtaglOrdered.pickle', 'rb') \n",
    "ftagl=pickle.load(f)\n",
    "f= open('BigExpanded.pickle', 'rb') \n",
    "expanded_routes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data structures (skip if serializied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.zeros(shape=[len(expanded_routes),30,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Transform raw route information into LSTM structure with dimensionality reduction\n",
    "## Note: Feature Agglomeration was done on all days\n",
    "\n",
    "row=0\n",
    "numEmpty=0\n",
    "df_rows=np.zeros(shape=(len(expanded_routes),2874))\n",
    "for seq in expanded_routes:\n",
    "    vec=np.zeros(shape=[30,2874])\n",
    "    df_vec=np.zeros(shape=[1,2874])\n",
    "    there=0\n",
    "    for i in range(30):\n",
    "        if seq.get(i):\n",
    "            vec[i][seq[i]]=1\n",
    "            df_vec[0][seq[i]]=1\n",
    "            there+=1\n",
    "\n",
    "            \n",
    "    vec=ftagl.transform(vec)\n",
    "    df_rows[row]=df_vec\n",
    "    X[row]=vec\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftagl.fit(df_rows)\n",
    "#with open('BigFtaglOrdered.pickle', 'wb') as output:\n",
    "#    pickle.dump(ftagl,output,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df_rows)\n",
    "del df_rows\n",
    "del expanded_routes\n",
    "df=pd.concat([pd.read_parquet('BigExtractedFeatures.parquet'),df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[str(x) for x in df.columns]\n",
    "df.to_parquet('universe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('X',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load serialized data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df.drop_duplicates()\n",
    "df=df[df['NumHops']>2]\n",
    "df=df.reset_index(drop=True)\n",
    "df['Benign'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Benign'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_half=df[df.Benign == False]\n",
    "back_half=back_half[round(0.8*len(back_half)):]\n",
    "good_len=round(0.8*len(df[df.Benign==True]))\n",
    "back_half=pd.concat([back_half,df[df.Benign== True][good_len:]],axis=0)\n",
    "print(back_half['Benign'].value_counts())\n",
    "\n",
    "front_half=df[df.Benign == False]\n",
    "front_half=front_half[0:round(0.8*len(front_half))]\n",
    "front_half=pd.concat([front_half,df[df.Benign == True][0:good_len]],axis=0)\n",
    "print(front_half['Benign'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction if desired\n",
    "\n",
    "# Fit new dim reduction matrix, not necessary if *ftagl*.pickle exists\n",
    "#ftagl = cluster.FeatureAgglomeration(n_clusters=100)\n",
    "\n",
    "reduced = ftagl.transform(df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process  and balance data\n",
    "\n",
    "#df=df.drop(columns=df.columns.difference([header for header in df.columns if not (str(header).isdigit())]))\n",
    "#reduced=pd.DataFrame(reduced)\n",
    "#df=pd.concat([df,reduced],axis=1)\n",
    "#del reduced\n",
    "\n",
    "\n",
    "bd=df[df['Benign'] == True]\n",
    "even=len(bd)\n",
    "md=df[df['Benign'] == False]\n",
    "md=md.reset_index(drop=True)\n",
    "\n",
    "bd=bd.sample(frac=1).reset_index(drop=True)\n",
    "md=md.sample(frac=1).reset_index(drop=True)\n",
    "md=md.loc[md.index < even]\n",
    "ad=pd.concat([bd,md])\n",
    "\n",
    "del bd\n",
    "del md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd=front_half[front_half['Benign'] == True]\n",
    "even=len(bd)\n",
    "md=front_half[front_half['Benign'] == False]\n",
    "md=md.reset_index(drop=True)\n",
    "\n",
    "bd=bd.sample(frac=1).reset_index(drop=True)\n",
    "md=md.sample(frac=1).reset_index(drop=True)\n",
    "md=md.loc[md.index < even]\n",
    "ad=pd.concat([bd,md])\n",
    "\n",
    "bd_v=back_half[back_half['Benign'] == True]\n",
    "even=len(bd_v)\n",
    "md_v=back_half[back_half['Benign'] == False]\n",
    "md_v=md_v.reset_index(drop=True)\n",
    "\n",
    "bd_v=bd_v.sample(frac=1).reset_index(drop=True)\n",
    "md_v=md_v.sample(frac=1).reset_index(drop=True)\n",
    "md_v=md_v.loc[md_v.index < even]\n",
    "ad_v=pd.concat([bd_v,md_v])\n",
    "\n",
    "del bd\n",
    "del md\n",
    "del bd_v\n",
    "del md_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude=ad.columns.difference(['indicator','Benign','Dest','Route','index']+['Timeouts','AveragePing','NumHops','Tail Timeouts','Reached'])\n",
    "exclude=ad.columns.difference(['indicator','Benign','Dest','Route','index'])\n",
    "ad=ad.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_dim=2879),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a sigmoid layer for outputs:\n",
    "layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#cease=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)\n",
    "history=model.fit(ad[exclude],ad['Benign'], epochs=100, verbose=1, batch_size=1024,\n",
    "          validation_data=(ad_v[exclude],ad_v['Benign']), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(ad['Benign'], model.predict(ad[exclude])>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(ad['Benign'], model.predict(ad[exclude]) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LSTM \n",
    "\n",
    "#### I think the shortfalls of the LSTM actually have to do with the dimensionality reduction which is absolutely necessary with this quantity of data.  \n",
    "\n",
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data, labels, dimReduction, batch_size=64, dim=(30,50), n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.expanded_routes=data\n",
    "        self.benign_labels=labels\n",
    "        self.index=0\n",
    "        self.dimReduction=dimReduction\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return int(np.floor(len(self.expanded_routes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self,ignore=True):\n",
    "\n",
    "        X=np.zeros(shape=[self.batch_size,30,2874])\n",
    "        row=0\n",
    "        for seq in expanded_routes[self.index:self.index+self.batch_size]:\n",
    "            vec=np.zeros(shape=[30,2874])\n",
    "           \n",
    "            for i in range(30):\n",
    "                if seq.get(i):\n",
    "                    vec[i][seq[i]]=1\n",
    "                    \n",
    "            X[row]=vec\n",
    "            row+=1\n",
    "            \n",
    "        self.index+=self.batch_size\n",
    "        return X, self.benign_labels[self.index-self.batch_size:self.index]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        #if self.shuffle == True:\n",
    "            #np.random.seed(387562875)\n",
    "            #np.random.shuffle(self.expanded_routes)\n",
    "            #np.random.seed(387562875)\n",
    "            #np.random.shuffle(self.benign_labels)\n",
    "        \n",
    "        self.index=0\n",
    "\n",
    "    def __data_generation(self,rand=None):\n",
    "\n",
    "        # Initialization\n",
    "        X=np.random.rand(self.batch_size,30,50)\n",
    "        y = np.round(np.random.rand((self.batch_size)))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "keep=df['NumHops']>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define empty training vars to be populated later\n",
    "\n",
    "X=np.load('X.npz')['arr_0']\n",
    "X=X[keep]\n",
    "Y=np.array(df['Benign'])\n",
    "Y=Y[keep]\n",
    "X_e=np.array(df[['Timeouts','AveragePing','NumHops','Tail Timeouts','Reached']])\n",
    "X_e=X_e[keep]\n",
    "df=df[keep]\n",
    "df=df.reset_index(drop=True)\n",
    "print(X.shape, Y.shape, X_e.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map benign and malicious entries for balancing purposes\n",
    "\n",
    "benignIndices=[]\n",
    "maliciousIndices=[]\n",
    "for x in range(0,len(df)):\n",
    "    if df['Benign'][x] == True:\n",
    "        benignIndices.append(x)\n",
    "    else:\n",
    "        maliciousIndices.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data from equal parts benign and malicious data, shuffle to avoid proximity effects\n",
    "\n",
    "X_benign_e=X_e[benignIndices,:]\n",
    "X_benign=X[benignIndices,:]\n",
    "Y_benign=Y[benignIndices]\n",
    "X_mal=X[maliciousIndices,:]\n",
    "X_mal_e=X_e[maliciousIndices,:]\n",
    "Y_mal=Y[maliciousIndices]\n",
    "del X\n",
    "del Y\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_mal)\n",
    "X_mal_e=X_mal_e[0:len(X_benign),:]\n",
    "X_mal=X_mal[0:len(X_benign),:]\n",
    "Y_mal=Y_mal[0:len(X_benign)]\n",
    "X_t_e=np.concatenate((X_benign_e,X_mal_e), axis=0)\n",
    "X_t=np.concatenate((X_benign,X_mal), axis=0)\n",
    "Y_t=np.concatenate((Y_benign,Y_mal), axis=0)\n",
    "del X_benign\n",
    "del X_benign_e\n",
    "del X_mal\n",
    "del X_mal_e\n",
    "del Y_benign\n",
    "del Y_mal\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                752384    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 760,769\n",
      "Trainable params: 760,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds an LSTM layer to intake the traceroute as a sequence of stops:\n",
    "layers.LSTM(64, activation='relu', input_shape=(30,2874), return_sequences=False),\n",
    "# Add dense layers to further interpret results:\n",
    "layers.Dense(64, activation='relu'),\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a sigmoid output layer\n",
    "layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvals=pd.read_parquet('BigExtractedFeatures.parquet')['Benign']\n",
    "mal_routes=pd.Series(expanded_routes)[~tvals]\n",
    "np.random.shuffle(mal_routes)\n",
    "mal_routes=mal_routes[0:sum(tvals)]\n",
    "new_expanded=pd.Series(expanded_routes)[tvals].append(mal_routes)\n",
    "del mal_routes\n",
    "tvals=[True]*sum(tvals)+[False]*sum(tvals)\n",
    "expanded_routes=new_expanded\n",
    "del new_expanded\n",
    "expanded_routes=expanded_routes.reset_index(drop=True)\n",
    "np.random.seed(29837492)\n",
    "np.random.shuffle(expanded_routes)\n",
    "np.random.seed(29837492)\n",
    "np.random.shuffle(tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "93/93 [==============================] - 69s 740ms/step - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6939 - val_acc: 0.4219\n",
      "Epoch 2/25\n",
      "93/93 [==============================] - 69s 740ms/step - loss: 0.6931 - acc: 0.5049 - val_loss: 0.6986 - val_acc: 0.4219\n",
      "Epoch 3/25\n",
      "93/93 [==============================] - 69s 737ms/step - loss: 0.6917 - acc: 0.5211 - val_loss: 0.7120 - val_acc: 0.4688\n",
      "Epoch 4/25\n",
      "93/93 [==============================] - 65s 696ms/step - loss: 0.6875 - acc: 0.5374 - val_loss: 0.6701 - val_acc: 0.5469\n",
      "Epoch 5/25\n",
      "93/93 [==============================] - 65s 694ms/step - loss: 0.6812 - acc: 0.5471 - val_loss: 0.6558 - val_acc: 0.5625\n",
      "Epoch 6/25\n",
      "93/93 [==============================] - 63s 676ms/step - loss: 0.6749 - acc: 0.5521 - val_loss: 0.6377 - val_acc: 0.6719\n",
      "Epoch 7/25\n",
      "93/93 [==============================] - 51s 549ms/step - loss: 0.6687 - acc: 0.5597 - val_loss: 0.6521 - val_acc: 0.5000\n",
      "Epoch 8/25\n",
      "93/93 [==============================] - 50s 540ms/step - loss: 0.6640 - acc: 0.5646 - val_loss: 0.6572 - val_acc: 0.5312\n",
      "Epoch 9/25\n",
      "93/93 [==============================] - 50s 539ms/step - loss: 0.6603 - acc: 0.5651 - val_loss: 0.6547 - val_acc: 0.5156\n",
      "Epoch 10/25\n",
      "93/93 [==============================] - 50s 541ms/step - loss: 0.6566 - acc: 0.5691 - val_loss: 0.6550 - val_acc: 0.4844\n",
      "Epoch 11/25\n",
      "93/93 [==============================] - 50s 541ms/step - loss: 0.6533 - acc: 0.5708 - val_loss: 0.6606 - val_acc: 0.4844\n",
      "Epoch 12/25\n",
      "93/93 [==============================] - 49s 530ms/step - loss: 0.6526 - acc: 0.5739 - val_loss: 0.6633 - val_acc: 0.4375\n",
      "Epoch 13/25\n",
      "93/93 [==============================] - 50s 534ms/step - loss: 0.6473 - acc: 0.5701 - val_loss: 0.6573 - val_acc: 0.4375\n",
      "Epoch 14/25\n",
      "93/93 [==============================] - 50s 538ms/step - loss: 0.6458 - acc: 0.5663 - val_loss: 0.6538 - val_acc: 0.5938\n",
      "Epoch 15/25\n",
      "93/93 [==============================] - 50s 537ms/step - loss: 0.6453 - acc: 0.5701 - val_loss: 0.6470 - val_acc: 0.6250\n",
      "Epoch 16/25\n",
      "93/93 [==============================] - 51s 543ms/step - loss: 0.6440 - acc: 0.5657 - val_loss: 0.6434 - val_acc: 0.5938\n",
      "Epoch 17/25\n",
      "93/93 [==============================] - 50s 538ms/step - loss: 0.6444 - acc: 0.5618 - val_loss: 0.6423 - val_acc: 0.6094\n",
      "Epoch 18/25\n",
      "93/93 [==============================] - 50s 540ms/step - loss: 0.6425 - acc: 0.5599 - val_loss: 0.6144 - val_acc: 0.6562\n",
      "Epoch 19/25\n",
      "93/93 [==============================] - 49s 530ms/step - loss: 0.6463 - acc: 0.5554 - val_loss: 0.6122 - val_acc: 0.6250\n",
      "Epoch 20/25\n",
      "93/93 [==============================] - 51s 543ms/step - loss: 0.6385 - acc: 0.5580 - val_loss: 0.5973 - val_acc: 0.6562\n",
      "Epoch 21/25\n",
      "93/93 [==============================] - 50s 539ms/step - loss: 0.6338 - acc: 0.5579 - val_loss: 0.6075 - val_acc: 0.6562\n",
      "Epoch 22/25\n",
      "93/93 [==============================] - 50s 539ms/step - loss: 0.6307 - acc: 0.5616 - val_loss: 0.6120 - val_acc: 0.6562\n",
      "Epoch 23/25\n",
      "93/93 [==============================] - 50s 537ms/step - loss: 0.6277 - acc: 0.5667 - val_loss: 0.6006 - val_acc: 0.6875\n",
      "Epoch 24/25\n",
      "93/93 [==============================] - 50s 538ms/step - loss: 0.6278 - acc: 0.5670 - val_loss: 0.6201 - val_acc: 0.6406\n",
      "Epoch 25/25\n",
      "93/93 [==============================] - 50s 534ms/step - loss: 0.6281 - acc: 0.5696 - val_loss: 0.6067 - val_acc: 0.6562\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#history=model.fit(X_t,Y_t, epochs=50, verbose=1, batch_size=64,\n",
    "          #validation_split=0.2, shuffle=True)\n",
    "\n",
    "trainingGen=DataGenerator(expanded_routes,tvals,False,batch_size=256)\n",
    "validationGen=DataGenerator(expanded_routes[0:100],tvals[0:100],False)\n",
    "history=model.fit_generator(generator=trainingGen,\n",
    "                            validation_data=validationGen,use_multiprocessing=True,epochs=25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.random.random((1000, 32))\n",
    "#labels = random_one_hot_labels((1000, 10))\n",
    "\n",
    "model.evaluate(X_t, Y_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(Y_t, model.predict(X_t)>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_t, model.predict(X_t) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Hybrid LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_t_e.shape, X_t.shape, Y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Two input sequences, the first is standard statistics about the traceroute and the second the sequence of stops \n",
    "my_inputs = layers.Input(shape=(5,),dtype='float32')\n",
    "ip_inputs = layers.Input(shape=(30,50),dtype='float32')\n",
    "\n",
    "# LSTM layer\n",
    "lstm_out = layers.LSTM(64, activation='relu')(ip_inputs)\n",
    "\n",
    "# Merge outputs from the LSTM layer with the next set of inputs\n",
    "x = keras.layers.concatenate([lstm_out, my_inputs])\n",
    "\n",
    "# Add dense layers to combine LSTM information with other data\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Sigmoid output layer\n",
    "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = models.Model(inputs=[my_inputs,ip_inputs], outputs=predictions)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#board=keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([X_t_e,X_t], Y_t, epochs=20, verbose =1, batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_t\n",
    "del Y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder \n",
    "\n",
    "#### Reduce dimensionality of subnet data\n",
    "#### Failed experiment: it appears the the data is too sparse for the sutoencoder to learn anything.  The output is always the same set of 4 or 5 entries predicted as ones regardless of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df[df['NumHops']>2]\n",
    "df=df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])]\n",
    "df=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df.sum(axis=1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 256\n",
    "\n",
    "# input vec\n",
    "input_vec = layers.Input(shape=(2874,))\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(x)\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(encoded)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(x)\n",
    "\n",
    "# \"decoded\" is the reconstruction of the input\n",
    "decoded = layers.Dense(2874, activation='softmax')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = models.Model(input_vec, decoded)\n",
    "autoencoder.summary()\n",
    "#encoder = models.Model(input_vec, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "#encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "#decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "#decoder = models.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.train.AdamOptimizer(.001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "autoencoder.fit(df,df,\n",
    "                epochs=1,\n",
    "                batch_size=4096,\n",
    "                shuffle=True,\n",
    "                validation_data=(df,df),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=autoencoder.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(pred).drop_duplicates()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
