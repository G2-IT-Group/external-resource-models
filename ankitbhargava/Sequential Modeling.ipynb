{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks and Sequential Modeling of Traceroutes\n",
    "\n",
    "Treating the traceroute as a sequence of steps towards a destination could reveal more nuanced patterns. Accuracy with these models can be as high as 80% but the hypothesis that additional information may be encoded in the order was disproven as the LSTM had no additional value when compared with the standard neural nets.  Therefore, the only thing that matters is which subnets are used, not necessarily in which order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import socket, struct\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, backend, models\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1634879994399725057\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16809275859740138054\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('BigFtaglOrdered.pickle', 'rb') \n",
    "ftagl=pickle.load(f)\n",
    "f= open('BigExpanded.pickle', 'rb') \n",
    "expanded_routes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data structures (skip if serializied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.zeros(shape=[len(expanded_routes),30,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Transform raw route information into LSTM structure with dimensionality reduction\n",
    "## Note: Feature Agglomeration was done on all days\n",
    "\n",
    "row=0\n",
    "numEmpty=0\n",
    "df_rows=np.zeros(shape=(len(expanded_routes),2874))\n",
    "for seq in expanded_routes:\n",
    "    vec=np.zeros(shape=[30,2874])\n",
    "    df_vec=np.zeros(shape=[1,2874])\n",
    "    there=0\n",
    "    for i in range(30):\n",
    "        if seq.get(i):\n",
    "            vec[i][seq[i]]=1\n",
    "            df_vec[0][seq[i]]=1\n",
    "            there+=1\n",
    "\n",
    "            \n",
    "    vec=ftagl.transform(vec)\n",
    "    df_rows[row]=df_vec\n",
    "    X[row]=vec\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftagl.fit(df_rows)\n",
    "#with open('BigFtaglOrdered.pickle', 'wb') as output:\n",
    "#    pickle.dump(ftagl,output,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df_rows)\n",
    "del df_rows\n",
    "del expanded_routes\n",
    "df=pd.concat([pd.read_parquet('BigExtractedFeatures.parquet'),df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[str(x) for x in df.columns]\n",
    "df.to_parquet('universe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('X',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load serialized data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    442357\n",
       "True      22255\n",
       "Name: Benign, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df.drop_duplicates()\n",
    "\n",
    "#Drop routes with 1 hop\n",
    "df=df[df['NumHops']>2]\n",
    "df=df.reset_index(drop=True)\n",
    "df['Benign'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    442357\n",
       "True      22255\n",
       "Name: Benign, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Benign'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate temporally if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_half=df[df.Benign == False]\n",
    "back_half=back_half[round(0.8*len(back_half)):]\n",
    "good_len=round(0.8*len(df[df.Benign==True]))\n",
    "back_half=pd.concat([back_half,df[df.Benign== True][good_len:]],axis=0)\n",
    "print(back_half['Benign'].value_counts())\n",
    "\n",
    "front_half=df[df.Benign == False]\n",
    "front_half=front_half[0:round(0.8*len(front_half))]\n",
    "front_half=pd.concat([front_half,df[df.Benign == True][0:good_len]],axis=0)\n",
    "print(front_half['Benign'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Net (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction if desired\n",
    "\n",
    "# Fit new dim reduction matrix, not necessary if *ftagl*.pickle exists\n",
    "#ftagl = cluster.FeatureAgglomeration(n_clusters=100)\n",
    "\n",
    "reduced = ftagl.transform(df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance data which if NOT dividing temporally\n",
    "\n",
    "#df=df.drop(columns=df.columns.difference([header for header in df.columns if not (str(header).isdigit())]))\n",
    "#reduced=pd.DataFrame(reduced)\n",
    "#df=pd.concat([df,reduced],axis=1)\n",
    "#del reduced\n",
    "\n",
    "\n",
    "bd=df[df['Benign'] == True]\n",
    "even=len(bd)\n",
    "md=df[df['Benign'] == False]\n",
    "md=md.reset_index(drop=True)\n",
    "\n",
    "bd=bd.sample(frac=1).reset_index(drop=True)\n",
    "md=md.sample(frac=1).reset_index(drop=True)\n",
    "md=md.loc[md.index < even]\n",
    "ad=pd.concat([bd,md])\n",
    "\n",
    "del bd\n",
    "del md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance temporally divided data\n",
    "\n",
    "bd=front_half[front_half['Benign'] == True]\n",
    "even=len(bd)\n",
    "md=front_half[front_half['Benign'] == False]\n",
    "md=md.reset_index(drop=True)\n",
    "\n",
    "bd=bd.sample(frac=1).reset_index(drop=True)\n",
    "md=md.sample(frac=1).reset_index(drop=True)\n",
    "md=md.loc[md.index < even]\n",
    "ad=pd.concat([bd,md])\n",
    "\n",
    "bd_v=back_half[back_half['Benign'] == True]\n",
    "even=len(bd_v)\n",
    "md_v=back_half[back_half['Benign'] == False]\n",
    "md_v=md_v.reset_index(drop=True)\n",
    "\n",
    "bd_v=bd_v.sample(frac=1).reset_index(drop=True)\n",
    "md_v=md_v.sample(frac=1).reset_index(drop=True)\n",
    "md_v=md_v.loc[md_v.index < even]\n",
    "ad_v=pd.concat([bd_v,md_v])\n",
    "\n",
    "del bd\n",
    "del md\n",
    "del bd_v\n",
    "del md_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature columns and shuffle dataframe\n",
    "exclude=ad.columns.difference(['indicator','Benign','Dest','Route','index'])\n",
    "ad=ad.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_dim=2879),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a sigmoid layer for outputs:\n",
    "layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35608 samples, validate on 8902 samples\n",
      "Epoch 1/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.6112 - acc: 0.6952 - val_loss: 0.5841 - val_acc: 0.7065\n",
      "Epoch 2/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.5606 - acc: 0.7149 - val_loss: 0.5306 - val_acc: 0.7292\n",
      "Epoch 3/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.5076 - acc: 0.7499 - val_loss: 0.4895 - val_acc: 0.7622\n",
      "Epoch 4/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.4721 - acc: 0.7753 - val_loss: 0.4771 - val_acc: 0.7620\n",
      "Epoch 5/25\n",
      "35608/35608 [==============================] - 47s 1ms/step - loss: 0.4504 - acc: 0.7869 - val_loss: 0.4537 - val_acc: 0.7772\n",
      "Epoch 6/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.4338 - acc: 0.7971 - val_loss: 0.4443 - val_acc: 0.7821\n",
      "Epoch 7/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.4206 - acc: 0.8041 - val_loss: 0.4365 - val_acc: 0.7896\n",
      "Epoch 8/25\n",
      "35608/35608 [==============================] - 47s 1ms/step - loss: 0.4097 - acc: 0.8111 - val_loss: 0.4294 - val_acc: 0.7921\n",
      "Epoch 9/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.4012 - acc: 0.8139 - val_loss: 0.4329 - val_acc: 0.7972\n",
      "Epoch 10/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3953 - acc: 0.8180 - val_loss: 0.4283 - val_acc: 0.7995\n",
      "Epoch 11/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3888 - acc: 0.8198 - val_loss: 0.4275 - val_acc: 0.8012\n",
      "Epoch 12/25\n",
      "35608/35608 [==============================] - 47s 1ms/step - loss: 0.3848 - acc: 0.8235 - val_loss: 0.4226 - val_acc: 0.8050\n",
      "Epoch 13/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3796 - acc: 0.8237 - val_loss: 0.4194 - val_acc: 0.7989\n",
      "Epoch 14/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3754 - acc: 0.8284 - val_loss: 0.4187 - val_acc: 0.8044\n",
      "Epoch 15/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3709 - acc: 0.8293 - val_loss: 0.4145 - val_acc: 0.8069\n",
      "Epoch 16/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3686 - acc: 0.8308 - val_loss: 0.4179 - val_acc: 0.8068\n",
      "Epoch 17/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3659 - acc: 0.8318 - val_loss: 0.4209 - val_acc: 0.8075\n",
      "Epoch 18/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3637 - acc: 0.8338 - val_loss: 0.4146 - val_acc: 0.8069\n",
      "Epoch 19/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3602 - acc: 0.8351 - val_loss: 0.4161 - val_acc: 0.8100\n",
      "Epoch 20/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3583 - acc: 0.8359 - val_loss: 0.4165 - val_acc: 0.8090\n",
      "Epoch 21/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3588 - acc: 0.8350 - val_loss: 0.4299 - val_acc: 0.8021\n",
      "Epoch 22/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3544 - acc: 0.8378 - val_loss: 0.4226 - val_acc: 0.8068\n",
      "Epoch 23/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3522 - acc: 0.8393 - val_loss: 0.4169 - val_acc: 0.8102\n",
      "Epoch 24/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3529 - acc: 0.8364 - val_loss: 0.4292 - val_acc: 0.8032\n",
      "Epoch 25/25\n",
      "35608/35608 [==============================] - 46s 1ms/step - loss: 0.3521 - acc: 0.8395 - val_loss: 0.4272 - val_acc: 0.8027\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#cease=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)\n",
    "\n",
    "## Train with temporally divided data\n",
    "#history=model.fit(ad[exclude],ad['Benign'], epochs=100, verbose=1, batch_size=1024,\n",
    "#         validation_data=(ad_v[exclude],ad_v['Benign']), shuffle=True)\n",
    "\n",
    "## Train with data NOT divided temporally\n",
    "history=model.fit(ad[exclude],ad['Benign'], epochs=25, verbose=1, batch_size=64,\n",
    "          validation_split=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.72      0.78      4413\n",
      "        True       0.76      0.89      0.82      4489\n",
      "\n",
      "    accuracy                           0.80      8902\n",
      "   macro avg       0.81      0.80      0.80      8902\n",
      "weighted avg       0.81      0.80      0.80      8902\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[3990  499]\n",
      " [1257 3156]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(ad['Benign'][int(len(ad)*0.8):], model.predict(ad[exclude][int(len(ad)*0.8):])>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(ad['Benign'][int(len(ad)*0.8):], model.predict(ad[exclude][int(len(ad)*0.8):]) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['indicator', 'Benign', 'Timeouts', 'AveragePing', 'NumHops',\n",
       "       'Tail Timeouts', 'Dest', 'Reached', '0', '1',\n",
       "       ...\n",
       "       '2864', '2865', '2866', '2867', '2868', '2869', '2870', '2871', '2872',\n",
       "       '2873'],\n",
       "      dtype='object', length=2882)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LSTM \n",
    "\n",
    "#### I think the shortfalls of the LSTM actually had to do with the dimensionality reduction which is absolutely necessary with this quantity of data.  Although trying with a data generator on the raw data did not improve results\n",
    "\n",
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a data generator to prevent memory overflow and increase training speed\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, data, labels, dimReduction, batch_size=64, dim=(30,50), n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.expanded_routes=data\n",
    "        self.benign_labels=labels\n",
    "        self.index=0\n",
    "        self.dimReduction=dimReduction\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return int(np.floor(len(self.expanded_routes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self,ignore=True):\n",
    "\n",
    "        # Return a 3d array represneting the one-hot encoding of subnets for batch_size number of rows\n",
    "        X=np.zeros(shape=[self.batch_size,30,2874])\n",
    "        row=0\n",
    "        for seq in expanded_routes[self.index:self.index+self.batch_size]:\n",
    "            vec=np.zeros(shape=[30,2874])\n",
    "           \n",
    "            for i in range(30):\n",
    "                if seq.get(i):\n",
    "                    vec[i][seq[i]]=1\n",
    "                    \n",
    "            X[row]=vec\n",
    "            row+=1\n",
    "            \n",
    "        self.index+=self.batch_size\n",
    "        return X, self.benign_labels[self.index-self.batch_size:self.index]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        if self.shuffle == True:\n",
    "            self.expanded_routes.reset_index(drop=True,inplace=True)\n",
    "            np.random.seed(387562875)\n",
    "            np.random.shuffle(self.expanded_routes)\n",
    "            np.random.seed(387562875)\n",
    "            np.random.shuffle(self.benign_labels)\n",
    "        \n",
    "        self.index=0\n",
    "\n",
    "    def __data_generation(self,rand=None):\n",
    "\n",
    "        # Initialization\n",
    "        X=np.random.rand(self.batch_size,30,50)\n",
    "        y = np.round(np.random.rand((self.batch_size)))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foloowing 4 data preparation steps are unnecessary if using data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "keep=df['NumHops']>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define empty training vars to be populated later\n",
    "\n",
    "X=np.load('X.npz')['arr_0']\n",
    "X=X[keep]\n",
    "Y=np.array(df['Benign'])\n",
    "Y=Y[keep]\n",
    "X_e=np.array(df[['Timeouts','AveragePing','NumHops','Tail Timeouts','Reached']])\n",
    "X_e=X_e[keep]\n",
    "df=df[keep]\n",
    "df=df.reset_index(drop=True)\n",
    "print(X.shape, Y.shape, X_e.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map benign and malicious entries for balancing purposes\n",
    "\n",
    "benignIndices=[]\n",
    "maliciousIndices=[]\n",
    "for x in range(0,len(df)):\n",
    "    if df['Benign'][x] == True:\n",
    "        benignIndices.append(x)\n",
    "    else:\n",
    "        maliciousIndices.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data from equal parts benign and malicious data, shuffle to avoid proximity effects\n",
    "\n",
    "X_benign_e=X_e[benignIndices,:]\n",
    "X_benign=X[benignIndices,:]\n",
    "Y_benign=Y[benignIndices]\n",
    "X_mal=X[maliciousIndices,:]\n",
    "X_mal_e=X_e[maliciousIndices,:]\n",
    "Y_mal=Y[maliciousIndices]\n",
    "del X\n",
    "del Y\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_mal_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_mal)\n",
    "X_mal_e=X_mal_e[0:len(X_benign),:]\n",
    "X_mal=X_mal[0:len(X_benign),:]\n",
    "Y_mal=Y_mal[0:len(X_benign)]\n",
    "X_t_e=np.concatenate((X_benign_e,X_mal_e), axis=0)\n",
    "X_t=np.concatenate((X_benign,X_mal), axis=0)\n",
    "Y_t=np.concatenate((Y_benign,Y_mal), axis=0)\n",
    "del X_benign\n",
    "del X_benign_e\n",
    "del X_mal\n",
    "del X_mal_e\n",
    "del Y_benign\n",
    "del Y_mal\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(X_t_e)\n",
    "np.random.seed(387562875)\n",
    "np.random.shuffle(Y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                752384    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 760,769\n",
      "Trainable params: 760,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "# Adds an LSTM layer to intake the traceroute as a sequence of stops:\n",
    "layers.LSTM(64, activation='relu', input_shape=(30,2874), return_sequences=False),\n",
    "# Add dense layers to further interpret results:\n",
    "layers.Dense(64, activation='relu'),\n",
    "    \n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a sigmoid output layer\n",
    "layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance subnet data that will be fed into LSTM, required for data generator\n",
    "\n",
    "tvals=pd.read_parquet('BigExtractedFeatures.parquet')['Benign']\n",
    "mal_routes=pd.Series(expanded_routes)[~tvals]\n",
    "np.random.shuffle(mal_routes)\n",
    "mal_routes=mal_routes[0:sum(tvals)]\n",
    "new_expanded=pd.Series(expanded_routes)[tvals].append(mal_routes)\n",
    "del mal_routes\n",
    "tvals=[True]*sum(tvals)+[False]*sum(tvals)\n",
    "expanded_routes=new_expanded\n",
    "del new_expanded\n",
    "expanded_routes=expanded_routes.reset_index(drop=True)\n",
    "np.random.seed(29837492)\n",
    "np.random.shuffle(expanded_routes)\n",
    "np.random.seed(29837492)\n",
    "np.random.shuffle(tvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopped training at 5 because it started to overfit after this point in previous iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "187/187 [==============================] - 103s 553ms/step - loss: 0.6842 - acc: 0.6414\n",
      "Epoch 2/5\n",
      "187/187 [==============================] - 105s 564ms/step - loss: 0.5582 - acc: 0.7190\n",
      "Epoch 3/5\n",
      "187/187 [==============================] - 105s 562ms/step - loss: 0.4830 - acc: 0.7639\n",
      "Epoch 4/5\n",
      "187/187 [==============================] - 106s 565ms/step - loss: 0.4572 - acc: 0.7776\n",
      "Epoch 5/5\n",
      "187/187 [==============================] - 107s 570ms/step - loss: 0.4481 - acc: 0.7805\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.0001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train using static data\n",
    "#history=model.fit(X_t,Y_t, epochs=50, verbose=1, batch_size=64,\n",
    "          #validation_split=0.2, shuffle=True)\n",
    "\n",
    "# Train and test using generators\n",
    "# *** Warning: Unresolved issues with wildly fluctuating training accuracy here\n",
    "trainingGen=DataGenerator(expanded_routes,tvals,False,batch_size=256)\n",
    "validationGen=DataGenerator(expanded_routes[0:1000],tvals[0:1000],False)\n",
    "history=model.fit_generator(generator=trainingGen,use_multiprocessing=True,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.random.random((1000, 32))\n",
    "#labels = random_one_hot_labels((1000, 10))\n",
    "\n",
    "model.evaluate(X_t, Y_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s\\n\"\n",
    "      % (metrics.classification_report(Y_t, model.predict(X_t)>.5)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_t, model.predict(X_t) > .5,labels=[True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Hybrid LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + Dense neural net which exploits all available features and processes subnets as a sequence\n",
    "#### No better than dense neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_t_e.shape, X_t.shape, Y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Two input sequences, the first is standard statistics about the traceroute and the second the sequence of stops \n",
    "my_inputs = layers.Input(shape=(5,),dtype='float32')\n",
    "ip_inputs = layers.Input(shape=(30,50),dtype='float32')\n",
    "\n",
    "# LSTM layer\n",
    "lstm_out = layers.LSTM(64, activation='relu')(ip_inputs)\n",
    "\n",
    "# Merge outputs from the LSTM layer with the next set of inputs\n",
    "x = keras.layers.concatenate([lstm_out, my_inputs])\n",
    "\n",
    "# Add dense layers to combine LSTM information with other data\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Sigmoid output layer\n",
    "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = models.Model(inputs=[my_inputs,ip_inputs], outputs=predictions)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#board=keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([X_t_e,X_t], Y_t, epochs=20, verbose =1, batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_t\n",
    "del Y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder \n",
    "\n",
    "#### Reduce dimensionality of subnet data\n",
    "#### No useful consolidation of features: it appears the the data is too sparse for the sutoencoder to learn anything.  The output is always the same set of 4 or 5 entries predicted as ones regardless of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('universe.parquet')\n",
    "df=df[df['NumHops']>2]\n",
    "df=df[df.columns.difference([header for header in df.columns if not (str(header).isdigit())])]\n",
    "df=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df.sum(axis=1)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 256\n",
    "\n",
    "# input vec\n",
    "input_vec = layers.Input(shape=(2874,))\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu')(x)\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_vec)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(encoded)\n",
    "\n",
    "x=layers.Dense(512,activation='relu',input_dim=(256,))(x)\n",
    "\n",
    "# \"decoded\" is the reconstruction of the input\n",
    "decoded = layers.Dense(2874, activation='softmax')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = models.Model(input_vec, decoded)\n",
    "autoencoder.summary()\n",
    "#encoder = models.Model(input_vec, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "#encoded_input = layers.Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "#decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "#decoder = models.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.train.AdamOptimizer(.001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "autoencoder.fit(df,df,\n",
    "                epochs=1,\n",
    "                batch_size=4096,\n",
    "                shuffle=True,\n",
    "                validation_data=(df,df),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=autoencoder.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(pred).drop_duplicates()\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
