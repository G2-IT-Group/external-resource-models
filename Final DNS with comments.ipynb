{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following is the code commented For Dave and John to read "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following loads needed imports so the code runs smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to note that sklearn.cross validation is commented out, as depending on the version of tensorflow\n",
    "sklearn.model_selection may be the correct choice. The way to know this is when the user runs the model\n",
    "and gives that one 'model_selection' doesn't exist, the user should switch to the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the purpose of this program is to parse through all the \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import re \n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import statistics\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following goes through every element of the /data/data folder and then creates a list  of the json, it also keeps track of the total ammounts that you are using. It uses the random function to allow you to get a random sampling of the jsons. This allows you to your models on a smaller percentage of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have here the inital path directory, because the folder is called /data/data, we use this. \n",
    "# Note we didn't include the last / after the second data because it will be included in later\n",
    "initial_path = \"/data/data\"\n",
    "# this is used so we can go through all the folders in the directory\n",
    "root_dir = os.listdir(initial_path)\n",
    "# this is used so we have a list of the json files we created\n",
    "list_of_json = []\n",
    "# this is the total ammount of the jsons we are dealing with. It's a good things for sanity checks.\n",
    "counter_total = 0;\n",
    "# in this for loop we go through every directory in /data/data , this is often a data of the same length\n",
    "for dir in root_dir:\n",
    "    # this is the current path that we go through. We add /enrichment/dns because that's the subfolder in the \n",
    "    # data folder of the path that it's in\n",
    "    curr_path = initial_path + \"/\" + dir + \"/enrichment/dns\"\n",
    "    # the following print statement was to keep track of the fact that when testing it actually went through all the jsons.\n",
    "    #print(curr_dir)\n",
    "    # We have an if else, because sometimes there may be no data. So we have to skip those cases\n",
    "    if os.path.exists(curr_path):\n",
    "        # we create a variable to go through each sub directory\n",
    "        curr_dir = os.listdir(curr_path)\n",
    "        # we have a for loop to go through each json file in the current direcotry\n",
    "        for sub_json in curr_dir:\n",
    "        #do i need the last /\n",
    "            # the json part for each sub_json only contains something like 123.json, so we need another variable to keep track\n",
    "            # of the full path name\n",
    "            each_json = curr_path + \"/\" + sub_json\n",
    "            print(each_json)\n",
    "            # we create a random object assigned to the word ran, this allows us to get a random sampling\n",
    "            ran =random.random()\n",
    "            # the random number is adjustable. If we want all the jsons, we can use a number larger than 1\n",
    "            # as random gives a number between 0 and 1  aka [0,1)\n",
    "            if(ran<.06):\n",
    "                # we keep track of the counter, this is also useful for debugging purposes\n",
    "                counter_total = counter_total+1\n",
    "                #print('counter_total'+ str(counter_total))\n",
    "                # we append each json to the list of jsons that we will use.\n",
    "                list_of_json.append(each_json)\n",
    "    else:\n",
    "        # we pass because we don't want to do anything\n",
    "        pass    \n",
    "print('counter_total'+ str(counter_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following alls you to append the jsons into a dataframe, note that the performance is O(n^2), because when you append a dataframe it creates a copy. A more efficient way to do this would be create a list of dataframe. This portion of the code is often the bottleneck. The code uses the random function twice to gie balanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list for the total data\n",
    "data_total = []\n",
    "# this is a dataframe that will hold our data from the jsons\n",
    "a_df = pd.DataFrame()\n",
    "# this\n",
    "target_function_list_origional_copy = []\n",
    "list_of_bad_json = []\n",
    "for a_json in list_of_json:\n",
    "    print('Test')\n",
    "    with open(''+a_json) as data_file:\n",
    "        try:             \n",
    "            data = json.load(data_file)\n",
    "            data_frame = json_normalize(data['data'])\n",
    "            print ((len(a_json)))\n",
    "           # print ((len(data_frame.index)))\n",
    "            if((len(a_json)<52)==True):\n",
    "                ran = random.random()\n",
    "                if( ran < .8):\n",
    "                    print('goooooooddddd')\n",
    "                    for x in range(0,len(data_frame.index),1):\n",
    "                        target_function_list_origional_copy.append(1)\n",
    "                    a_df = a_df.append(data_frame)\n",
    "            if((len(a_json)>=52)==True):\n",
    "                ran = random.random()\n",
    "                if( ran < .4):\n",
    "                    print('badd')\n",
    "                    for x in range(0,len(data_frame.index),1):\n",
    "                        target_function_list_origional_copy.append(0)             \n",
    "                    a_df = a_df.append(data_frame)\n",
    "            #print(\"target_function_list length\" + len(target_function_list))\n",
    "\n",
    "        except:\n",
    "           \n",
    "            list_of_bad_json.append(a_json)\n",
    "            continue\n",
    "print(\"done\" )        \n",
    "print(len(a_df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
